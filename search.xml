<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>AA sort</title>
      <link href="/post/AA-sort/"/>
      <url>/post/AA-sort/</url>
      
        <content type="html"><![CDATA[<p>Aligned-Access sort (AA-sort) 访问对齐排序，可支持SIMD和多线程的排序算法。</p><p><a href="PACT2007-SIMDsort.pdf">AA-sort: A new parallel sorting algorithm for multi-core SIMD processors</a></p><p>算法主要流程分三步：</p><ul><li>(1) Divide all of the data into blocks that fit into the cache of the processor.</li><li>(2) Sort each block with the in-core sorting algorithm.</li><li>(3) Merge the sorted blocks with the out-of-core algorithm. First we present these two sorting algorithms and then illustrate the overall sorting scheme.</li></ul><span id="more"></span><h2 id="1-Divide-data："><a href="#1-Divide-data：" class="headerlink" title="1. Divide data："></a>1. Divide data：</h2><p>切分数据的依据是Cache能存下的数据量为一个batch，在文章中，作者使用了L2Cache&#x2F;2作为参考值。</p><h2 id="2-In-core-algorithm：Vectorized-Combsort"><a href="#2-In-core-algorithm：Vectorized-Combsort" class="headerlink" title="2. In-core algorithm：Vectorized Combsort"></a>2. In-core algorithm：<a href="https://zh.wikipedia.org/zh-cn/%E6%A2%B3%E6%8E%92%E5%BA%8F">Vectorized Combsort</a></h2><p>Combsort梳子排序是冒泡排序的优化版，由gap&#x3D;n&#x2F;k开始依次比较交换a[i], a[i+gap]，然后gap&#x3D;gap&#x2F;k重复，直到gap&#x3D;1，一直冒泡直到排好序；一般取k&#x3D;1.24或者1.3。</p><p><img src="/Comb_sort_demo.gif"></p><p>combsort中，当gap较大时，是符合SIMD优化原则的，但是当gap&lt;4时，就需要寄存器内不同通道的数据相互比较了。</p><p>对此，作者提出将原数据reshape(-1,4)后进行排序的方法：</p><p><img src="/aasort-reshape.png"></p><p>如上图，假设上方original order是希望得到的排序好的数据，下面Transposed order是reshape(-1,4)后的数据，将数据显示成矩阵形式则成为左下角的样子，观察这个矩阵，对于每一列，都是排序好的，而每一列的数都比下一列的数要小。</p><p>而在SIMD架构中，得到左下角这个矩阵则比要直接得到original order要简单，因为最直接的大小关系从左右相邻的数比较，变成了上下寄存器之间比较，而这是SIMD所支持的。因此只要从原始数据得到左下角的矩阵，就能通过重排得到排序好的数据。</p><p><strong>in-core排序过程</strong>：</p><p>对每组4个数据排序，确保v0&lt;v1&lt;v2&lt;v3<br>以寄存器为单位的梳子排序：主要流程即梳子排序的过程，数与数的比较替换成寄存器的比较vector_cmpswap；除此之外还需要增加一些操作以确保每一列的数都比后一列的数要小的关系；方法是在对一个gap完成一次排序后，依次对后gap组数据和前gap组数据执行操作vector_cmpswap_skew；最终达到排序完成状态。伪代码如下图所示：</p><p><img src="/incore.png"><br><img src="/vector_cmpswap.png"></p><p>vector_cmpswap：直接比较交换两个寄存器对应通道的值。<br>vector_cmpswap_skew：比较交换两个寄存器错位通道的值，大小关系与vector_cmpswap相反。<br>重排数据，排序完成。</p><h2 id="3-out-of-core-algorithm：Merge"><a href="#3-out-of-core-algorithm：Merge" class="headerlink" title="3. out-of-core algorithm：Merge"></a>3. out-of-core algorithm：Merge</h2><p>in-core algorithm后，得到了M组排好序的数据；out-of-core algorithm做的是将他们合并；</p><p>合并流程类似于普通数组合并，两个头指针分别指向两个数组，数据较大的指针pop数据后移，直到两个数组完全合并；只不过在这里，单位是寄存器。</p><p>合并算法过程（以递减排序为例）：从两组数据中读取数据到两个寄存器，两寄存器合并重排得到较小一半数据和较大的一半数据，比较合并的过程如下图所示，较大的数据用作输出，较小的数据放寄存器等待下一次比较；然后输出数据的寄存器从两组剩余数据中选择一组读取新的待合并数据，选择的依据是首位数据较大的；然后不断重复以上过程。</p><p><img src="/outof_core.png"></p><p>mark：</p><p>并行排序算法 <a href="https://stackoverflow.com/questions/3969813/which-parallel-sorting-algorithm-has-the-best-average-case-performance">https://stackoverflow.com/questions/3969813/which-parallel-sorting-algorithm-has-the-best-average-case-performance</a></p>]]></content>
      
      
      <categories>
          
          <category> 性能优化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 向量化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分支预测损耗分析</title>
      <link href="/post/Cpu-Pipeline/"/>
      <url>/post/Cpu-Pipeline/</url>
      
        <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p><strong>CPU组成</strong>：控制器CU（Control Unit）：指令寄存器IR(InstructionRegister)、程序计数器PC(ProgramCounter)、操作控制器OC(OperationController)；算数逻辑运算单元ALU；寄存器。</p><p><strong>指令流水线</strong>（Instruction pipeline）：计算机指令在CPU内部会会被转译成一个或多个微指令（uop），这些uop在不同的cpu模块中依次执行。如果CPU一次处理一个指令，那么某个模块在执行时，其他模块都处于空闲状态，所以CPU会将多条指令以流水线的形式执行，以充分利用CPU模块提高吞吐来提高多指令执行速度，即指令流水线。</p><span id="more"></span><p><strong>指令流水线级数</strong>(depth)：一个指令被分成多少个阶段执行。</p><p><strong>CPU时钟周期</strong>：流水线最慢一级的处理时间（逻辑处理+流水线寄存器）</p><p><strong>流水线延迟</strong>：完整的指令处理时间（指令流水线级数*CPU时钟周期）</p><p><strong>吞吐量</strong>：每秒可处理最小单位指令数（G instruction per second）（1s&#x2F;CPU时钟周期）</p><p>如三级流水线的CPU架构中，三条指令I1、I2、I3能够在4个时钟周期完成：</p><p><img src="/3LevelCpuArchitecture.png"></p><h1 id="分支预测"><a href="#分支预测" class="headerlink" title="分支预测"></a>分支预测</h1><p>CPU处理分支时会先预测将会执行的指令，然后将对应的指令放进流水线，但可能会预测错误，导致流水线清空。</p><h1 id="分支预测错误成本"><a href="#分支预测错误成本" class="headerlink" title="分支预测错误成本"></a>分支预测错误成本</h1><p>记D&#x3D;流水线深度，K&#x3D;指令预测错误出现的深度，则分支预测错误的成本为（K-1）时钟周期。所以流水线级数越高，分支预测错误带来的损耗越大。</p><p>预测错误演示和分析如下：</p><p>以4级流水线为例，4条一般指令（不同颜色）可以在8个时钟周期内通过不同的阶段并执行完。</p><p><img src="/PipelineNormal.png"></p><p>而如果绿色跟紫色指令有依赖关系，如分支依赖、数据依赖，则在第3个时钟周期，本应该译码的紫色需要等待绿色执行完后再进行，增加的气泡表示指令延迟，相当于nop；第4时钟周期时，绿色执行后，紫色才恢复执行。此时指令执行完成额外消耗1个时钟周期。</p><p><img src="/PipelineDelay.png"></p><p>而如果绿色跟紫色指令是分支（控制）依赖关系，且分支预测错误了，则会将当前流水线清空，并将正确分支的指令重新放进流水线执行；如Clock4，新的三条指令被重新放进pipeline重新开始执行。最终指令执行完成额外消耗3个时钟周期。</p><p><img src="/PipelinePredFault.png"></p><h1 id="分支预测器"><a href="#分支预测器" class="headerlink" title="分支预测器"></a>分支预测器</h1><p><strong>分支預測器</strong>（英語：Branch predictor）是一種數位電路，在分支指令執行結束之前猜測哪一路分支將會被執行，以提高處理器的指令管線的效能。</p><p><strong>動態預測</strong>：利用分支指令發生轉移的歷史來進行預測，並根據實際執行情況動態調整預測位，準確率可達90%，現在幾乎所有處理器都採用動態預測。</p><p>详细的分支预测器介绍：<a href="https://zh.wikipedia.org/wiki/%E5%88%86%E6%94%AF%E9%A0%90%E6%B8%AC%E5%99%A8">分支预测器介绍</a></p><h1 id="C-关键字-builtin-expect"><a href="#C-关键字-builtin-expect" class="headerlink" title="C++关键字 __builtin_expect"></a>C++关键字 __builtin_expect</h1><p>likely&#x2F;unlikely作用于编译器，编译器将更可能的分支指令放在分支后面，而更不可能的分支指令需要跳转执行，这样CPU分支预测器在预测的初期会更倾向于预测顺序执行的指令；而当CPU分支预测器的样本足够时会自动调整该分支预测结果。所以likely&#x2F;unlikely对分支刚开始执行时有帮助，基本上还是依赖于CPU的分支预测器。</p><blockquote><p>Many CPUs have a branch predictor, thus using these macros only helps the first time code is executed or when the history table is overwritten by a different branch with the same index into the branching table. <a href="https://stackoverflow.com/a/109721">stackoverflow</a></p></blockquote><h1 id="REFER"><a href="#REFER" class="headerlink" title="REFER"></a>REFER</h1><p><a href="https://zh.wikipedia.org/wiki/%E6%8C%87%E4%BB%A4%E7%AE%A1%E7%B7%9A%E5%8C%96">wiki:指令管線化</a></p><p><a href="L11-Pipelined-Datapath-And.pdf">L11-Pipelined-Datapath-And.pdf</a></p><p><strong>部分微架构流水线级数</strong></p> <table class="wrapped confluenceTable" style="letter-spacing: 0.0px;"><colgroup><col /><col /></colgroup><thead><tr><th class="confluenceTh" style="text-align: center;">微架構<p>（Microarchitecture）</p></th><th class="confluenceTh" style="text-align: center;">管線層數<p>（Pipeline stages）</p></th></tr></thead><tbody><tr><td class="confluenceTd">Sony Cell</td><td class="confluenceTd">23</td></tr><tr><td class="confluenceTd">IBM PowerPC 7</td><td class="confluenceTd">17</td></tr><tr><td class="confluenceTd">IBM Xenon</td><td class="confluenceTd">19</td></tr><tr><td class="confluenceTd">AMD Athlon</td><td class="confluenceTd">10</td></tr><tr><td class="confluenceTd">AMD&nbsp;<a class="mw-redirect" title="Athlon XP" href="https://zh.wikipedia.org/wiki/Athlon_XP">Athlon XP</a></td><td class="confluenceTd">11</td></tr><tr><td class="confluenceTd">AMD&nbsp;<a class="mw-redirect" title="Athlon 64" href="https://zh.wikipedia.org/wiki/Athlon_64">Athlon 64</a></td><td class="confluenceTd">12</td></tr><tr><td class="confluenceTd">AMD&nbsp;<a class="mw-redirect" title="Phenom" href="https://zh.wikipedia.org/wiki/Phenom">Phenom</a></td><td class="confluenceTd">12</td></tr><tr><td class="confluenceTd">AMD&nbsp;<a title="Opteron" href="https://zh.wikipedia.org/wiki/Opteron">Opteron</a></td><td class="confluenceTd">15</td></tr><tr><td class="confluenceTd">ARM7TDMI (-S)</td><td class="confluenceTd">3</td></tr><tr><td class="confluenceTd">ARM7EJ-S</td><td class="confluenceTd">5</td></tr><tr><td class="confluenceTd">ARM810</td><td class="confluenceTd">5</td></tr><tr><td class="confluenceTd">ARM9TDMI</td><td class="confluenceTd">5</td></tr><tr><td class="confluenceTd">ARM1020E</td><td class="confluenceTd">6</td></tr><tr><td class="confluenceTd">XScale PXA210/PXA250</td><td class="confluenceTd">7</td></tr><tr><td class="confluenceTd">ARM1136J (F)-S</td><td class="confluenceTd">8</td></tr><tr><td class="confluenceTd">ARM1156T2 (F)-S</td><td class="confluenceTd">9</td></tr><tr><td class="confluenceTd">ARM Cortex-A5</td><td class="confluenceTd">8</td></tr><tr><td class="confluenceTd">ARM Cortex-A8</td><td class="confluenceTd">13</td></tr><tr><td class="confluenceTd">AVR32 AP7</td><td class="confluenceTd">7</td></tr><tr><td class="confluenceTd">AVR32 UC3</td><td class="confluenceTd">3</td></tr><tr><td class="confluenceTd"><a class="new" href="https://zh.wikipedia.org/w/index.php?title=DLX&amp;action=edit&amp;redlink=1">DLX</a></td><td class="confluenceTd">5</td></tr><tr><td class="confluenceTd"><a class="mw-redirect" title="Intel P5" href="https://zh.wikipedia.org/wiki/Intel_P5">Intel P5</a>（<a class="mw-redirect" title="Pentium" href="https://zh.wikipedia.org/wiki/Pentium">Pentium</a>）</td><td class="confluenceTd">5</td></tr><tr><td class="confluenceTd"><a class="mw-redirect" title="Intel P6" href="https://zh.wikipedia.org/wiki/Intel_P6">Intel P6</a>（<a class="mw-redirect" title="Pentium Pro" href="https://zh.wikipedia.org/wiki/Pentium_Pro">Pentium Pro</a>）</td><td class="confluenceTd">14</td></tr><tr><td class="confluenceTd">Intel P6（<a class="mw-redirect" title="Pentium III" href="https://zh.wikipedia.org/wiki/Pentium_III">Pentium III</a>）</td><td class="confluenceTd">10</td></tr><tr><td class="confluenceTd"><a class="mw-redirect" title="Intel NetBurst" href="https://zh.wikipedia.org/wiki/Intel_NetBurst">Intel NetBurst</a>（Willamette）</td><td class="confluenceTd">20</td></tr><tr><td class="confluenceTd">Intel NetBurst（Northwood）</td><td class="confluenceTd">20</td></tr><tr><td class="confluenceTd">Intel NetBurst（Prescott）</td><td class="confluenceTd">31</td></tr><tr><td class="confluenceTd">Intel NetBurst（Cedar Mill）</td><td class="confluenceTd">31</td></tr><tr><td class="confluenceTd"><a class="mw-redirect" title="Intel Core微處理器架構" href="https://zh.wikipedia.org/wiki/Intel_Core%E5%BE%AE%E8%99%95%E7%90%86%E5%99%A8%E6%9E%B6%E6%A7%8B">Intel Core</a></td><td class="confluenceTd">14</td></tr><tr><td class="confluenceTd"><a class="mw-redirect" title="Intel Atom" href="https://zh.wikipedia.org/wiki/Intel_Atom">Intel Atom</a></td><td class="confluenceTd">16</td></tr><tr><td class="confluenceTd">LatticeMico32</td><td class="confluenceTd">6</td></tr><tr><td class="confluenceTd">R4000</td><td class="confluenceTd">8</td></tr><tr><td class="confluenceTd">StrongARM SA-110</td><td class="confluenceTd">5</td></tr><tr><td class="confluenceTd">SuperH SH2</td><td class="confluenceTd">5</td></tr><tr><td class="confluenceTd">SuperH SH2A</td><td class="confluenceTd">5</td></tr><tr><td class="confluenceTd">SuperH SH4</td><td class="confluenceTd">5</td></tr><tr><td class="confluenceTd">SuperH SH4A</td><td class="confluenceTd">7</td></tr><tr><td class="confluenceTd"><a class="mw-redirect" title="UltraSPARC" href="https://zh.wikipedia.org/wiki/UltraSPARC">UltraSPARC</a></td><td class="confluenceTd">9</td></tr><tr><td class="confluenceTd"><a title="UltraSPARC T1" href="https://zh.wikipedia.org/wiki/UltraSPARC_T1">UltraSPARC T1</a></td><td class="confluenceTd">6</td></tr><tr><td class="confluenceTd"><a title="UltraSPARC T2" href="https://zh.wikipedia.org/wiki/UltraSPARC_T2">UltraSPARC T2</a></td><td class="confluenceTd">8</td></tr><tr><td class="confluenceTd"><a class="new" href="https://zh.wikipedia.org/w/index.php?title=WinChip&amp;action=edit&amp;redlink=1">WinChip</a></td><td class="confluenceTd">4</td></tr><tr><td class="confluenceTd"><a class="new" href="https://zh.wikipedia.org/w/index.php?title=LC2200_32_bit&amp;action=edit&amp;redlink=1">LC2200 32 bit</a></td><td class="confluenceTd">5</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 性能优化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SIMD优化</title>
      <link href="/post/SIMD-Optimization/"/>
      <url>/post/SIMD-Optimization/</url>
      
        <content type="html"><![CDATA[<h1 id="向量化计算"><a href="#向量化计算" class="headerlink" title="向量化计算"></a>向量化计算</h1><p>向量化计算是一种特殊的并行计算的方式，相比于一般程序在同一时间只执行一个操作的方式，它可以在同一时间执行多次操作，通常是对不同的数据执行同样的一个或一批指令，或者说把指令应用于一个数组&#x2F;向量。</p><span id="more"></span><p><a href="https://eigen.tuxfamily.org/index.php?title=Main_Page">eigen</a></p><h1 id="SIMD"><a href="#SIMD" class="headerlink" title="SIMD"></a>SIMD</h1><p>Single instruction, multiple data 单指令多数据流</p><p>依赖于CPU的向量寄存器，一次指令对多个数据进行相同的操作。</p><ul><li>数据内存中连续储存</li><li>对于连续数据的操作相同或者规律相同</li><li>处理的数据间没有相互依赖关系</li><li>寄存器带宽比数据类型大。。数据对齐</li></ul><p><a href="https://software.intel.com/sites/landingpage/IntrinsicsGuide/">Intel-SSE</a> ， <a href="https://developer.arm.com/architectures/instruction-sets/simd-isas/neon">ARM-NEON</a></p><h1 id="SIMD代码改造优化的tips："><a href="#SIMD代码改造优化的tips：" class="headerlink" title="SIMD代码改造优化的tips："></a>SIMD代码改造优化的tips：</h1><h2 id="循环优化"><a href="#循环优化" class="headerlink" title="循环优化"></a>循环优化</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 连续内存数据循环在最内层</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; H; i++)</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; W; j++)</span><br><span class="line">        a[i,j]++;</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 考虑cache的循环拆分</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; H; i++)</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; W; j++)</span><br><span class="line">        a[i] = a[i] + b[j];</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; W; j += batch)</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; H; i++)</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> jj = j; jj &lt; j + batch; jj++)</span><br><span class="line">            a[i] = a[i] + b[jj]</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h2 id="移除分支"><a href="#移除分支" class="headerlink" title="移除分支"></a>移除分支</h2><p>关于分支预测失败的性能损耗可以参考这里（尽管分支预测成功率很高）</p><p>等效的数学公式替代分支</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> c = <span class="number">0</span>; c &lt; arraySize; c++)</span><br><span class="line">        <span class="keyword">if</span> (data[c] &gt;= <span class="number">128</span>)</span><br><span class="line">            sum += data[c]; </span><br><span class="line">    <span class="comment">// equivalent</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> c = <span class="number">0</span>; c &lt; arraySize; c++) &#123;</span><br><span class="line">        <span class="type">int</span> t = (data[c] - <span class="number">128</span>) &gt;&gt; <span class="number">31</span>;</span><br><span class="line">        sum += ~t &amp; data[c];</span><br><span class="line">    &#125;</span><br><span class="line"> <span class="comment">// -O2 if被优化成cmovge -&gt;  ADD sum_t data[c]</span></span><br><span class="line">                            ADD <span class="type">sum_t</span> <span class="number">-128</span></span><br><span class="line">                            cmovge sum <span class="type">sum_t</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">// cmp swap</span></span><br><span class="line">    <span class="keyword">if</span> (v1 &gt; v2) &#123; new_v = v1; new_i = i1; &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123; new_v = v2; new_i = i2; &#125;</span><br><span class="line"><span class="comment">// simd version</span></span><br><span class="line">    new_i = <span class="built_in">vbslq_f32</span>(<span class="built_in">vcgtq_f32</span>(v1, v2), i1, i2);   <span class="comment">// vcgt 比较a&gt;b则对应位置全置1，否则0； vbsl 如果对应位置1则取a，否则b； </span></span><br><span class="line">                                                    <span class="comment">// 类似于 bool ge = a &gt; b; int max = ge * a + !ge * b;</span></span><br><span class="line">    new_v = <span class="built_in">vmaxq_f32</span>(v1, v2);</span><br></pre></td></tr></table></figure><h2 id="数据重排"><a href="#数据重排" class="headerlink" title="数据重排"></a>数据重排</h2><p>必要的时候对数据进行重新排列，以配合SIMD达到性能提升。</p><h2 id="输出blob的reshape、对齐"><a href="#输出blob的reshape、对齐" class="headerlink" title="输出blob的reshape、对齐"></a>输出blob的reshape、对齐</h2><p>因为SIMD的特性，除了需要保证数据对齐128字节，而且要求数据内存连续；所以需要根据具体的算法流程确定好数据的内存排布，如网络输出应该是NCHW还是NHWC还是要NC4HW4对齐，在实现前需要确定清楚。</p><h2 id="数据处理前对读取的数据在寄存器内部重排"><a href="#数据处理前对读取的数据在寄存器内部重排" class="headerlink" title="数据处理前对读取的数据在寄存器内部重排"></a>数据处理前对读取的数据在寄存器内部重排</h2><p>有时候在需要处理相邻的数据时，使用SIMD可能达不到好的效果，这时候可以考虑将数据进行转置，比如AA-sort中进行in-core排序前对4x4的数据进行了转置，以便于后续的比较，见这里；其实就是reshape的衍生。</p><h2 id="消除数据存取延迟与数据依赖延迟"><a href="#消除数据存取延迟与数据依赖延迟" class="headerlink" title="消除数据存取延迟与数据依赖延迟"></a>消除数据存取延迟与数据依赖延迟</h2><p>由于CPU指令流水线等机制，消除指令间的依赖关系能够提高指令的执行效率</p><h2 id="通过数据预取减少内存读写延迟"><a href="#通过数据预取减少内存读写延迟" class="headerlink" title="通过数据预取减少内存读写延迟"></a>通过数据预取减少内存读写延迟</h2><p>数据预取能加快数据读取速度</p><h2 id="通过指令横向扩展抵消内存读写延迟"><a href="#通过指令横向扩展抵消内存读写延迟" class="headerlink" title="通过指令横向扩展抵消内存读写延迟"></a>通过指令横向扩展抵消内存读写延迟</h2><p>ld v1 → dosth v1 → st v1  指令可能因为内存读写阻塞</p><p>横向扩展 ld v1, v2 → dosth v1 ,v2→ st v1 ,v2   像这样一次处理两组，则内存阻塞等待时间相当于减少一半</p><h2 id="通过指令横向扩展消除数据依赖延迟"><a href="#通过指令横向扩展消除数据依赖延迟" class="headerlink" title="通过指令横向扩展消除数据依赖延迟"></a>通过指令横向扩展消除数据依赖延迟</h2><p>v2 &#x3D; dosth v1 → v3 &#x3D; dosth v2</p><p>横向扩展 v2, v22 &#x3D; dosth v1, v11→ v3, v33 &#x3D; dosth v2, v22</p><p>可以消除指令间的依赖关系，提高执行效率</p><h1 id="NEON"><a href="#NEON" class="headerlink" title="NEON"></a>NEON</h1><p>ARM cpu拓展结构，包括64-bit和32个128-bit的寄存器，以提供SIMD方式高效的图片视频数据处理能力。提供汇编代码和内联函数调用的使用方式。</p><p>寄存器结构：Elements are the standard Neon-supported widths of 8 (B), 16 (H), 32 (S), or 64 (D) bits.</p><p><img src="/RegisterArchitecture.png"></p><p>数据存取，除了提供数据顺序存取，还提供数据交叉存取，方便处理多通道的数据。</p><p><img src="https://developer.arm.com/-/media/Arm%20Developer%20Community/Images/Tutorial%20Guide%20Diagrams%20and%20Screenshots/Neon%20Programmers%20Guide/Coding%20for%20Neon/neon_load_ld1.PNG?revision=d4143976-1e7f-40c2-83be-5612bd80d4ac&la=en&hash=D97168DCC3A1E58D06033DBE29D344356E939AF1"></p><p><img src="https://developer.arm.com/-/media/Arm%20Developer%20Community/Images/Tutorial%20Guide%20Diagrams%20and%20Screenshots/Neon%20Programmers%20Guide/Coding%20for%20Neon/neon_load_ld3_st3.PNG?revision=3df85177-775b-449f-9b57-81db746b88d2&la=en&hash=1C13AEA90F773082A8065F7FD818AD574FDF255B"></p><p>其中内联函数仅提供从低位0开始填充寄存器的读取方式，而汇编可以指定寄存器偏移。如使用方式为：LD3 { V0.B, V1.B, V2.B }[4] , [x0], #48</p><p><img src="https://developer.arm.com/-/media/Arm%20Developer%20Community/Images/Tutorial%20Guide%20Diagrams%20and%20Screenshots/Neon%20Programmers%20Guide/Coding%20for%20Neon/neon_load_syntax.PNG?revision=882f16c1-6649-41a8-9503-6576a675efbd&la=en&hash=C3422E7D348DEAFF021E05802CF8A5DD97EEE39F"></p><h1 id="其他资料"><a href="#其他资料" class="headerlink" title="其他资料"></a>其他资料</h1><p>官方文档整理了NEON数据重排可以用到的一些指令：<a href="https://developer.arm.com/architectures/instruction-sets/simd-isas/neon/neon-programmers-guide-for-armv8-a/coding-for-neon/permutation-neon-instructions">https://developer.arm.com/architectures/instruction-sets/simd-isas/neon/neon-programmers-guide-for-armv8-a/coding-for-neon/permutation-neon-instructions</a></p><p>其他的内联函数指令查询：<a href="https://developer.arm.com/architectures/instruction-sets/simd-isas/neon/intrinsics?search=vtrn">https://developer.arm.com/architectures/instruction-sets/simd-isas/neon/intrinsics?search=vtrn</a></p><p>网友提供的内联指令中文介绍: <a href="https://github.com/rogerou/Arm-neon-intrinsics]">https://github.com/rogerou/Arm-neon-intrinsics]</a>(<a href="https://github.com/rogerou/Arm-neon-intrinsics">https://github.com/rogerou/Arm-neon-intrinsics</a></p><p>开发指南： <a href="https://developer.arm.com/documentation/den0018/a">https://developer.arm.com/documentation/den0018/a</a>  (离线版本：DEN0018A_neon_programmers_guide.pdf</p><p>ARM、neon汇编指令中文介绍：RealView汇编指南中文版.pdf</p>]]></content>
      
      
      <categories>
          
          <category> 性能优化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 向量化 </tag>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>杂</title>
      <link href="/post/TrashBin/"/>
      <url>/post/TrashBin/</url>
      
        <content type="html"><![CDATA[<p>待整理项</p><span id="more"></span><h1 id="TMUX"><a href="#TMUX" class="headerlink" title="TMUX"></a>TMUX</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tmux new-session -d -s monitor <span class="string">&#x27;top&#x27;</span> \; \</span><br><span class="line">  split-window -h <span class="string">&#x27;watch -n 1 nvidia-smi&#x27;</span> \; \</span><br><span class="line">  select-pane -t 1 \; \</span><br><span class="line">  split-window -v <span class="string">&#x27;watch -n 1  &quot;ps aux| head -1; ps aux | sort -rnk 4 | head -10&quot;&#x27;</span> \;</span><br><span class="line"> </span><br><span class="line">tmux a -t monitor</span><br></pre></td></tr></table></figure><h1 id="查找AVX-SSE-指令受哪个编译选项控制"><a href="#查找AVX-SSE-指令受哪个编译选项控制" class="headerlink" title="查找AVX SSE 指令受哪个编译选项控制"></a>查找AVX SSE 指令受哪个编译选项控制</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="title">get_instruction</span></span> ()</span><br><span class="line">&#123;</span><br><span class="line">    [ -z <span class="string">&quot;<span class="variable">$1</span>&quot;</span> ] &amp;&amp; <span class="built_in">exit</span></span><br><span class="line">    func_name=<span class="string">&quot;<span class="variable">$1</span> &quot;</span></span><br><span class="line"> </span><br><span class="line">    header_file=`grep --include=\*intrin.h -Rl <span class="string">&quot;<span class="variable">$func_name</span>&quot;</span> /usr/lib/gcc | <span class="built_in">head</span> -n1`</span><br><span class="line">    [ -z <span class="string">&quot;<span class="variable">$header_file</span>&quot;</span> ] &amp;&amp; <span class="built_in">exit</span></span><br><span class="line">    &gt;&amp;2 <span class="built_in">echo</span> <span class="string">&quot;find in: <span class="variable">$header_file</span>&quot;</span></span><br><span class="line"> </span><br><span class="line">    target_directive=`grep <span class="string">&quot;#pragma GCC target(\|<span class="variable">$func_name</span>&quot;</span> <span class="variable">$header_file</span> | grep -B 1 <span class="string">&quot;<span class="variable">$func_name</span>&quot;</span> | <span class="built_in">head</span> -n1`</span><br><span class="line">    <span class="built_in">echo</span> <span class="variable">$target_directive</span> | grep -o <span class="string">&#x27;&quot;[^,]*[,&quot;]&#x27;</span> | sed <span class="string">&#x27;s/&quot;//g&#x27;</span> | sed <span class="string">&#x27;s/,//g&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">instruction=`get_instruction <span class="variable">$1</span>`</span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$instruction</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Error: function not found: <span class="variable">$1</span>&quot;</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;add this option to gcc: -m<span class="variable">$instruction</span>&quot;</span></span><br><span class="line">    gcc -m<span class="variable">$instruction</span> -dM -E - &lt; /dev/null | egrep <span class="string">&quot;SSE|AVX&quot;</span> | <span class="built_in">sort</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><h1 id="valgrind"><a href="#valgrind" class="headerlink" title="valgrind"></a>valgrind</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#cd valgrind</span></span><br><span class="line"><span class="comment">#./configure --prefix=/usr/local/valgrind--指定安装目录</span></span><br><span class="line"><span class="comment">#make</span></span><br><span class="line"><span class="comment">#make install</span></span><br><span class="line"></span><br><span class="line">用法:valgrind[options] prog-and-args [options]: 常用选项，适用于所有Valgrind工具</span><br><span class="line">-tool=&lt;name&gt; 最常用的选项。运行 valgrind中名为toolname的工具。默认memcheck。</span><br><span class="line">h –<span class="built_in">help</span> 显示帮助信息。</span><br><span class="line">-version 显示valgrind内核的版本，每个工具都有各自的版本。</span><br><span class="line">q –quiet 安静地运行，只打印错误信息。</span><br><span class="line">v –verbose 更详细的信息, 增加错误数统计。</span><br><span class="line">-trace-children=no|<span class="built_in">yes</span> 跟踪子线程? [no]</span><br><span class="line">-track-fds=no|<span class="built_in">yes</span> 跟踪打开的文件描述？[no]</span><br><span class="line">-time-stamp=no|<span class="built_in">yes</span> 增加时间戳到LOG信息? [no]</span><br><span class="line">-log-fd=&lt;number&gt; 输出LOG到描述符文件 [2=stderr]</span><br><span class="line">-log-file=&lt;file&gt; 将输出的信息写入到filename.PID的文件里，PID是运行程序的进行ID</span><br><span class="line">-log-file-exactly=&lt;file&gt; 输出LOG信息到 file</span><br><span class="line">-log-file-qualifier=&lt;VAR&gt; 取得环境变量的值来做为输出信息的文件名。 [none]</span><br><span class="line">-log-socket=ipaddr:port 输出LOG到socket ，ipaddr:port</span><br><span class="line">LOG信息输出:</span><br><span class="line">-xml=<span class="built_in">yes</span> 将信息以xml格式输出，只有memcheck可用</span><br><span class="line">-num-callers=&lt;number&gt; show &lt;number&gt; callers <span class="keyword">in</span> stack traces [12]</span><br><span class="line">-error-limit=no|<span class="built_in">yes</span> 如果太多错误，则停止显示新错误? [<span class="built_in">yes</span>]</span><br><span class="line">-error-exitcode=&lt;number&gt; 如果发现错误则返回错误代码 [0=<span class="built_in">disable</span>]</span><br><span class="line">-db-attach=no|<span class="built_in">yes</span> 当出现错误，valgrind会自动启动调试器gdb。[no]</span><br><span class="line">-db-command=&lt;<span class="built_in">command</span>&gt; 启动调试器的命令行选项[gdb -nw %f %p]</span><br><span class="line">适用于Memcheck工具的相关选项：</span><br><span class="line">-leak-check=no|summary|full 要求对leak给出详细信息? [summary]</span><br><span class="line">-leak-resolution=low|med|high how much bt merging <span class="keyword">in</span> leak check [low]</span><br><span class="line">-show-reachable=no|<span class="built_in">yes</span> show reachable blocks <span class="keyword">in</span> leak check? [no]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow-Plan</title>
      <link href="/post/TensorFlow-Plan/"/>
      <url>/post/TensorFlow-Plan/</url>
      
        <content type="html"><![CDATA[<h1 id="TensorFlow-Example"><a href="#TensorFlow-Example" class="headerlink" title="TensorFlow-Example"></a>TensorFlow-Example</h1><p>从TensorFlow-Example上手TensorFlow和DL。</p><span id="more"></span><p>解读TensorFlow-Example，熟悉TensorFlow的同时认识各种深度学习模型，也同时补充其中遇到的各种概率信息论的硬核知识。以此做进度记录和学习笔记。</p><p>参考：</p><ul><li><p><a href="https://github.com/aymericdamien/TensorFlow-Examples/">TensorFlow-Examples</a></p></li><li><p><a href="https://github.com/exacity/deeplearningbook-chinese">deeplearning-book中文版</a></p></li><li><p><a href="https://github.com/jtoy/awesome-tensorflow">Awesome-Tensorflow</a> TensorFlow项目收集</p></li></ul><h2 id="基础篇"><a href="#基础篇" class="headerlink" title="基础篇"></a>基础篇</h2><ul><li><input checked="" disabled="" type="checkbox"> TensorFlow 基本流程结构 (<a href="../TensorFlow-Note">及记录笔记</a>)</li></ul><h2 id="工具篇"><a href="#工具篇" class="headerlink" title="工具篇"></a>工具篇</h2><ul><li><input checked="" disabled="" type="checkbox"> 保存加载模型 (两句)<ul><li>saver &#x3D; tf.train.Saver()</li><li>保存 save_path &#x3D; saver.save(sess, model_path) </li><li>加载 saver.restore(sess, model_path)</li></ul></li><li><input disabled="" type="checkbox"> tensorboard_basic</li><li><input disabled="" type="checkbox"> tensorboard_advanced</li></ul><h2 id="模型篇"><a href="#模型篇" class="headerlink" title="模型篇"></a>模型篇</h2><ul><li><input checked="" disabled="" type="checkbox"> linear_regression 线性回归 (<a href="../TensorFlow-Examples-LR">2019.2.21</a>)</li><li><input checked="" disabled="" type="checkbox"> logistic_regression 逻辑回归 (<a href="../TensorFlow-Examples-LOR">2019.2.22</a>)</li><li><input checked="" disabled="" type="checkbox"> nearest_neighbor 最近邻 (<a href="../TensorFlow-Examples-NN">2019.3.6</a>)</li><li><input checked="" disabled="" type="checkbox"> kmeans K均值聚类 (<a href="../TensorFlow-Examples-K-Means">2019.3.18</a>)</li><li><input disabled="" type="checkbox"> random_forest 随机森林 ([todo])</li><li><input disabled="" type="checkbox"> gradient_boosted_decision_tree 梯度提升决策树</li><li><input disabled="" type="checkbox"> word2vec 词向量</li></ul><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><ul><li><input disabled="" type="checkbox"> autoencoder 自编码器</li><li><input disabled="" type="checkbox"> variational_autoencoder 变分自编码器</li><li><input disabled="" type="checkbox"> convolutional_network 卷积神经网路</li><li><input disabled="" type="checkbox"> convolutional_network_raw ？</li><li><input disabled="" type="checkbox"> gan 生成式对抗网络</li><li><input disabled="" type="checkbox"> dcgan 深度卷积生成对抗网络</li><li><input disabled="" type="checkbox"> dynamic_rnn 动态递归神经网络</li><li><input disabled="" type="checkbox"> recurrent_network 递回神经网路</li><li><input disabled="" type="checkbox"> bidirectional_rnn 双向递回神经网路</li><li><input disabled="" type="checkbox"> multilayer_perceptron 多层感知机</li><li><input disabled="" type="checkbox"> neural_network 神经网络</li><li><input disabled="" type="checkbox"> neural_network_raw ？</li></ul><h2 id="有生之年"><a href="#有生之年" class="headerlink" title="有生之年"></a>有生之年</h2><ul><li><input disabled="" type="checkbox"> eager</li><li><input disabled="" type="checkbox"> dataset</li><li><input disabled="" type="checkbox"> GPU</li></ul>]]></content>
      
      
      <categories>
          
          <category> tensorflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>指定nmake编译架构</title>
      <link href="/post/nmake%E7%BC%96%E8%AF%91protobuf/"/>
      <url>/post/nmake%E7%BC%96%E8%AF%91protobuf/</url>
      
        <content type="html"><![CDATA[<p>修改VC默认架构以指定nmake编译项目的目标架构。</p><span id="more"></span><p>在使用nmake编译protobuf的过程中，出现了<code>LNK4272:库计算机类型“x64”与目标计算机类型“x86”冲突</code>的错误，执行过程：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; protobuf\cmake\build\release&gt;cmake -G &quot;NMake Makefiles&quot; -DCMAKE_BUILD_TYPE=Release -Dprotobuf_BUILD_TESTS=OFF -DCMAKE_INSTALL_PREFIX=../../../../install ../..</span><br><span class="line">    -- The C compiler identification is MSVC 19.23.28106.4</span><br><span class="line">    -- The CXX compiler identification is MSVC 19.23.28106.4</span><br><span class="line">    -- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.23.28105/bin/Hostx86/x86/cl.exe</span><br><span class="line">    -- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.23.28105/bin/Hostx86/x86/cl.exe -- works</span><br><span class="line">    ……</span><br><span class="line">&gt; protobuf\cmake\build\release&gt;nmake</span><br><span class="line">    C:\Program Files\PostgreSQL\9.4\lib\zlib.lib : warning LNK4272:库计算机类型“x64”与目标计算机类型“x86”冲突</span><br><span class="line">    tests.exe : fatal error LNK1120: 6 个无法解析的外部命令</span><br><span class="line">    NMAKE : fatal error U1077: “&quot;C:\Program Files\CMake\bin\cmake.exe&quot;”: 返回代码“0xffffffff”</span><br><span class="line">    Stop.</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>试着指定x64架构</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; protobuf\cmake\build\release&gt;cmake -G &quot;NMake Makefiles&quot; -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=../../../../install -A x64 ../..</span><br><span class="line">CMake Error at CMakeLists.txt:16 (project):</span><br><span class="line">  Generator</span><br><span class="line">    NMake Makefiles</span><br><span class="line">  does not support platform specification, but platform</span><br><span class="line">    x64</span><br><span class="line">  was specified.</span><br></pre></td></tr></table></figure><p>nmake不支持指定架构，需要修改VC默认编译架构</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Auxiliary\Build&gt;vcvarsall.bat x64</span><br><span class="line">**********************************************************************</span><br><span class="line">** Visual Studio 2019 Developer Command Prompt v16.3.5</span><br><span class="line">** Copyright (c) 2019 Microsoft Corporation</span><br><span class="line">**********************************************************************</span><br><span class="line">[vcvarsall.bat] Environment initialized for: &#x27;x64&#x27;</span><br></pre></td></tr></table></figure><p>删掉cmakeCache，重来</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; protobuf\cmake\build\release&gt;cmake -G &quot;NMake Makefiles&quot; -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=../../../../install -Dprotobuf_BUILD_TESTS=OFF ../..</span><br><span class="line">-- The C compiler identification is MSVC 19.23.28106.4</span><br><span class="line">-- The CXX compiler identification is MSVC 19.23.28106.4</span><br><span class="line">-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.23.28105/bin/Hostx64/x64/cl.exe</span><br><span class="line">-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.23.28105/bin/Hostx64/x64/cl.exe -- works</span><br></pre></td></tr></table></figure><p>确实从x86变成了x64，后续nmake成功。</p><p>至于为啥不用cmake生成VS项目sln，因为使用的版本<code>cmake version 3.13.0-rc1</code>还不支持VS2019…</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&gt; protobuf\cmake\build\solution&gt;cmake -G &quot;Visual Studio 16 2019&quot; -DCMAKE_INSTALL_PREFIX=../../../../install ../..</span><br><span class="line">CMake Error: Could not create named generator Visual Studio 16 2019</span><br><span class="line"></span><br><span class="line">&gt; protobuf\cmake\build\solution&gt;cmake -G &quot;Visual Studio 14 2015&quot; -DCMAKE_INSTALL_PREFIX=../../../../install ../..</span><br><span class="line">-- 这里甚至没有任何提示</span><br><span class="line"></span><br><span class="line">&gt; protobuf\cmake\build\solution&gt;cmake -G &quot;Visual Studio 15 2017&quot; -DCMAKE_INSTALL_PREFIX=../../../../install ../..</span><br><span class="line">-- Selecting Windows SDK version 10.0.17763.0 to target Windows 10.0.18362.</span><br><span class="line">CMake Error at CMakeLists.txt:16 (project):</span><br><span class="line">  Failed to run MSBuild command:</span><br><span class="line">    MSBuild.exe</span><br><span class="line">  to get the value of VCTargetsPath:</span><br><span class="line">    用于 .NET Framework 的 Microsoft (R) 生成引擎版本 16.3.1+1def00d3d</span><br><span class="line">    版权所有(C) Microsoft Corporation。保留所有权利。</span><br><span class="line">    生成启动时间为 2019/11/4 16:55:45。</span><br><span class="line">    节点 1 上的项目“\protobuf\cmake\build\solution\CMakeFiles\3.13.0-rc1\VCTargetsPath.vcxproj”(默认目标)。</span><br><span class="line">    C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\MSBuild\Microsoft\VC\v160\Microsoft.CppBuild.targets(379,5): error MSB8020: The build tools for Visual Studio 2017 (Platform Toolset = &#x27;v141&#x27;) cannot be found.</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>css</title>
      <link href="/post/css/"/>
      <url>/post/css/</url>
      
        <content type="html"><![CDATA[<span id="more"></span><h2 id="表格内容过长导致宽度超出页面"><a href="#表格内容过长导致宽度超出页面" class="headerlink" title="表格内容过长导致宽度超出页面"></a>表格内容过长导致宽度超出页面</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">css添加属性：</span><br><span class="line">table&#123;word-break:break-all;&#125;</span><br><span class="line"></span><br><span class="line">table-layout:设置表格的布局算法：</span><br><span class="line">属性值描述</span><br><span class="line">automatic       默认。列宽度由单元格内容设定。</span><br><span class="line">fixed           列宽由表格宽度和列宽度设定。</span><br><span class="line">inherit         规定应该从父元素继承 table-layout 属性的值。</span><br><span class="line">word-break：在合适的点换行</span><br><span class="line"></span><br><span class="line">word-break: normal|break-all|keep-all;</span><br><span class="line">值描述</span><br><span class="line">normal          使用浏览器默认的换行规则。</span><br><span class="line">break-all       允许在单词内换行。</span><br><span class="line">keep-all        只能在半角空格或连字符处换行。</span><br><span class="line">--------------------- </span><br><span class="line">原文：https://blog.csdn.net/buside/article/details/86528235 </span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>C++ 字节对齐导致的地址越界</title>
      <link href="/post/C-%E5%AD%97%E8%8A%82%E5%AF%B9%E9%BD%90%E5%AF%BC%E8%87%B4%E7%9A%84%E5%9C%B0%E5%9D%80%E8%B6%8A%E7%95%8C/"/>
      <url>/post/C-%E5%AD%97%E8%8A%82%E5%AF%B9%E9%BD%90%E5%AF%BC%E8%87%B4%E7%9A%84%E5%9C%B0%E5%9D%80%E8%B6%8A%E7%95%8C/</url>
      
        <content type="html"><![CDATA[<p>由于类字节对齐不一致导致访问成员时地址错乱，造成访问越界等问题。</p><span id="more"></span><h2 id="表现及背景"><a href="#表现及背景" class="headerlink" title="表现及背景"></a>表现及背景</h2><p>将程序从32位升级64位后，程序内存访问越界；</p><p>将A类作为入参调用一个函数，进入函数后A的成员变量就完全乱了（最开始还以为是编译器优化导致的监视变量不成功）；进而引发的0xffff访问出错等一系列问题。</p><h2 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h2><p>升级64位时，程序中使用的32位变量地址转换可能会出现异常，参考<a href="https://blog.csdn.net/xt18971492243/article/details/80945195">32位代码移植到64位需注意问题</a>等博客；但是分析了一波，出现异常时并没有在处理赋值、类型转换等操作，除非是调用函数时A的拷贝构造函数出错了，但是是引用传递，就算改为指针传递也出现同样问题。</p><p>进而猜测是编译器层面的错误，所以比较了64位和以前32位的项目属性设置，得不出结果，稍微留意了<code>/Zp 结构成员对齐</code>的设置，从默认改成了<code>16</code>和<code>8</code>，但依然出现问题（没有尝试<code>4</code>，不然可能可以更早发现问题）。</p><p><em>btw，根据<a href="https://docs.microsoft.com/zh-cn/cpp/build/reference/zp-struct-member-alignment?view=vs-2017">MS官网</a>，<code>16</code>是64位程序的默认对齐，而<code>8</code>是32位的默认对齐，但实际上64位8个byte应该8字节对齐？而32位则应该是4？</em> </p><p>后来想看看指针传递过程中指针是不是变了，却发现A类指针没变，但里面某一个成员变量的地址却变了，偏移了4个字节；一下子有了方向，并且找到了一个情况类似的<a href="https://www.oschina.net/question/2323945_2279721">全局对象成员地址改变的文章</a>；于是用文中提到的&#x2F;d1 reportSingleClassLayoutA编译选项去看A的内存分布，果然发现了在编译不同的cpp时，出现了一个不一样的结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">1&gt;A.cpp</span><br><span class="line">1&gt;class Asize(1088):</span><br><span class="line">1&gt;+---</span><br><span class="line">1&gt; 0| Config config_</span><br><span class="line">1&gt;784| last_store_</span><br><span class="line">1&gt;788| is_tried_</span><br><span class="line">1&gt;  | &lt;alignment member&gt; (size=3)</span><br><span class="line">1&gt;792| blocking_threshold_</span><br><span class="line">1&gt;  | &lt;alignment member&gt; (size=4)          &lt;---多了4字节对齐</span><br><span class="line">1&gt;800| index_table_header</span><br><span class="line">......</span><br><span class="line">1&gt;960| result_table_header</span><br><span class="line"></span><br><span class="line">&gt;B.cpp</span><br><span class="line">1&gt;class Asize(1084):</span><br><span class="line">1&gt;+---</span><br><span class="line">1&gt; 0| Config config_</span><br><span class="line">1&gt;784| last_store_</span><br><span class="line">1&gt;788| is_tried_</span><br><span class="line">1&gt;  | &lt;alignment member&gt; (size=3)</span><br><span class="line">1&gt;792| blocking_threshold_</span><br><span class="line">1&gt;796| index_table_header</span><br><span class="line">......</span><br><span class="line">1&gt;956| result_table_header</span><br><span class="line">1&gt;+---</span><br></pre></td></tr></table></figure><p>在编译A.cpp时，编译器对Class A的内存对齐比B.cpp多了4字节的预留，导致后面的所有成员变量都偏移了，数据自然就不对了，取其中string成员的时候就发生了内存越界。</p><p>最后是在全局搜索<code>#pragma pack</code>发现了根本原因，在项目使用的一个三方库中，有一个头文件使用了<code>#pragma pack(1)</code>但最后却使用<code>#pragma pack(4)</code>而不是<code>#pragma pack()</code>恢复默认，导致B.cpp在引用该头文件之后编译的对齐方式都改变了，导致了这样的错误。而原来32位程序中，本来对齐方式就是4字节，因此没有出现问题.</p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>MS官网上的32位默认8字节对齐、64位默认16字节对齐是忽悠的。</p>]]></content>
      
      
      <categories>
          
          <category> BUG </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow Examples K-Means</title>
      <link href="/post/TensorFlow-Examples-K-Means/"/>
      <url>/post/TensorFlow-Examples-K-Means/</url>
      
        <content type="html"><![CDATA[<p>从<a href="https://github.com/aymericdamien/TensorFlow-Examples/">TensorFlow-Examples</a>学习TensorFlow之：K-Means。<span id="more"></span><a href="../TensorFlow-Plan">进度</a></p><h1 id="BasicModels-K-Means-K均值聚类"><a href="#BasicModels-K-Means-K均值聚类" class="headerlink" title="BasicModels: K-Means K均值聚类"></a>BasicModels: K-Means K均值聚类</h1><p>kmeans计算步骤：</p><ul><li><p>随机选取k个中心点</p></li><li><p>循环：</p><ul><li><p>遍历所有数据，归类到最近的中心点，形成k个聚类</p></li><li><p>计算每个聚类中的均值，作为新的中心点</p></li></ul></li><li><p>直到: k个中心点不再变化</p></li></ul><p>样例使用KMEANS工厂类建立图表，所以建表关键代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kmeans = KMeans(inputs=X, num_clusters=k, distance_metric=&#x27;cosine&#x27;, use_mini_batch=True)</span><br><span class="line">training_graph = kmeans.training_graph()</span><br><span class="line">(all_scores, cluster_idx, scores, cluster_centers_initialized, init_op, train_op) = training_graph</span><br></pre></td></tr></table></figure><hr><p>样例代码 <a href="https://github.com/yezhenbang/TensorFlow-Examples/tree/parse">kmeans.py</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line">from __future__ import print_function</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.contrib.factorization import KMeans</span><br><span class="line"></span><br><span class="line"># Ignore all GPUs, tf k-means does not benefit from it.</span><br><span class="line">import os</span><br><span class="line">os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;&quot;</span><br><span class="line"></span><br><span class="line"># Import MNIST data</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">mnist = input_data.read_data_sets(&quot;/tmp/data/&quot;, one_hot=True)</span><br><span class="line">full_data_x = mnist.train.images</span><br><span class="line"></span><br><span class="line"># Parameters</span><br><span class="line">num_steps = 50 # Total steps to train 循环50次</span><br><span class="line">batch_size = 1024 # The number of samples per batch 无用</span><br><span class="line">k = 25 # The number of clusters 企图分出25个聚类</span><br><span class="line">num_classes = 10 # The 10 digits</span><br><span class="line">num_features = 784 # Each image is 28x28 pixels</span><br><span class="line"></span><br><span class="line"># Input images</span><br><span class="line">X = tf.placeholder(tf.float32, shape=[None, num_features])</span><br><span class="line"># Labels (for assigning a label to a centroid and testing)</span><br><span class="line">Y = tf.placeholder(tf.float32, shape=[None, num_classes])</span><br><span class="line"></span><br><span class="line"># K-Means Parameters</span><br><span class="line">kmeans = KMeans(inputs=X, num_clusters=k, distance_metric=&#x27;cosine&#x27;,</span><br><span class="line">                use_mini_batch=True)</span><br><span class="line"># KMeans class</span><br><span class="line"># __init__(</span><br><span class="line">#     inputs,               输入数据</span><br><span class="line">#     num_clusters,         分类数量</span><br><span class="line">#     initial_clusters=RANDOM_INIT,     初始中心点，可tensor、numpy指定、&quot;random&quot;、&quot;kmeans_plus_plus&quot;</span><br><span class="line">#           random 随机从输入抽取</span><br><span class="line">#     distance_metric=SQUARED_EUCLIDEAN_DISTANCE,   距离计算方式，Supported:&quot;squared_euclidean&quot;,&quot;cosine&quot;</span><br><span class="line">#     use_mini_batch=False,             对数据分块(batch)迭代，加快收敛，默认不开启，全数据迭代</span><br><span class="line">#     mini_batch_steps_per_iteration=1, （mini—batch模式下）多少次迭代后将子块训练数据更新到主副本（更新中心点）</span><br><span class="line">#     random_seed=0,                    随机化初始中心点的随机种子</span><br><span class="line">#     kmeans_plus_plus_num_retries=2,   （待深入）For each point that is sampled during kmeans++ initialization, this parameter specifies the number of additional points to draw from the current distribution before selecting the best. If a negative value is specified, a heuristic is used to sample O(log(num_to_sample)) additional points.</span><br><span class="line">#     kmc2_chain_length=200             （待深入）Determines how many candidate points are used by the k-MC2 algorithm to produce one new cluster centers. If a (mini-)batch contains less points, one new cluster center is generated from the (mini-)batch.</span><br><span class="line"># )</span><br><span class="line"></span><br><span class="line"># Build KMeans graph</span><br><span class="line"># 根据上面创建的KMEANS类的参数，training_graph()构造了k-means算法的graph，</span><br><span class="line"># 并返回各结点(tensor)。无需自己设计计算流程，真方便</span><br><span class="line">training_graph = kmeans.training_graph()</span><br><span class="line"># training_graph():</span><br><span class="line">#   return (all_scores, cluster_idx, scores, cluster_centers_initialized,</span><br><span class="line">#             init_op, training_op)</span><br><span class="line"># all_scores: 每个输入到每个聚类中心的距离矩阵 (len(X), k)</span><br><span class="line"># cluster_idx: 向量，对应每个输入最近的聚类中心id (len(X))，</span><br><span class="line"># scores: 向量，对应每个输入到最近的中心的距离 (len(X))</span><br><span class="line"># cluster_centers_initialized: 标量，中心是否已经初始化</span><br><span class="line"># init_op: 操作，初始化中心</span><br><span class="line"># training_op: 操作，训练</span><br><span class="line"></span><br><span class="line">if len(training_graph) &gt; 6: </span><br><span class="line">    (all_scores, cluster_idx, scores, cluster_centers_initialized,</span><br><span class="line">     cluster_centers_var, init_op, train_op) = training_graph</span><br><span class="line">else: #TensorFlow 1.13.1 从上面training_graph()的返回可以看到只有6个变量</span><br><span class="line">    (all_scores, cluster_idx, scores, cluster_centers_initialized,</span><br><span class="line">     init_op, train_op) = training_graph</span><br><span class="line"></span><br><span class="line">cluster_idx = cluster_idx[0] # fix for cluster_idx being a tuple</span><br><span class="line">avg_distance = tf.reduce_mean(scores)</span><br><span class="line"># scores的均值，用以表示整体匹配度</span><br><span class="line"></span><br><span class="line"># Initialize the variables (i.e. assign their default value)</span><br><span class="line">init_vars = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"># Start TensorFlow session</span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"># Run the initializer</span><br><span class="line">sess.run(init_vars, feed_dict=&#123;X: full_data_x&#125;)</span><br><span class="line">sess.run(init_op, feed_dict=&#123;X: full_data_x&#125;)</span><br><span class="line"></span><br><span class="line"># Training</span><br><span class="line">for i in range(1, num_steps + 1):</span><br><span class="line">    _, d, idx = sess.run([train_op, avg_distance, cluster_idx],</span><br><span class="line">                         feed_dict=&#123;X: full_data_x&#125;)</span><br><span class="line">    if i % 10 == 0 or i == 1:</span><br><span class="line">        print(&quot;Step %i, Avg Distance: %f&quot; % (i, d))</span><br><span class="line"></span><br><span class="line"># Assign a label to each centroid</span><br><span class="line"># Count total number of labels per centroid, using the label of each training</span><br><span class="line"># sample to their closest centroid (given by &#x27;idx&#x27;)</span><br><span class="line"># counts 通过每个输入样例和样例输出，得到每个聚类中代表10个数字的次数统计 (k, 10)</span><br><span class="line">counts = np.zeros(shape=(k, num_classes))</span><br><span class="line">for i in range(len(idx)):</span><br><span class="line">    counts[idx[i]] += mnist.train.labels[i]</span><br><span class="line"># Assign the most frequent label to the centroid</span><br><span class="line"># labels_map 向量，每个聚类出现最多的数字，作为聚类预测结果 (k)</span><br><span class="line">labels_map = [np.argmax(c) for c in counts]</span><br><span class="line">labels_map = tf.convert_to_tensor(labels_map)</span><br><span class="line"></span><br><span class="line"># Evaluation ops</span><br><span class="line"># Lookup: centroid_id -&gt; label</span><br><span class="line"># embedding_lookup将cluster_idx每一个元素作为下标在labels_map中查找，获取其值</span><br><span class="line"># 根据labels_map，将cluster_idx从对应每个输入最近中心坐标，变成每个输入预测结果</span><br><span class="line">cluster_label = tf.nn.embedding_lookup(labels_map, cluster_idx)</span><br><span class="line"># Compute accuracy</span><br><span class="line"># 准确率计算</span><br><span class="line">correct_prediction = tf.equal(cluster_label, tf.cast(tf.argmax(Y, 1), tf.int32))</span><br><span class="line">accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line"># Test Model</span><br><span class="line">test_x, test_y = mnist.test.images, mnist.test.labels</span><br><span class="line"># 计算测试用例，得到准确率</span><br><span class="line">print(&quot;Test Accuracy:&quot;, sess.run(accuracy_op, feed_dict=&#123;X: test_x, Y: test_y&#125;))</span><br><span class="line"></span><br></pre></td></tr></table></figure><hr><p>参考：</p><p><a href="https://www.cnblogs.com/bourneli/p/3645049.html">https://www.cnblogs.com/bourneli/p/3645049.html</a></p>]]></content>
      
      
      <categories>
          
          <category> tensorflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow-Examples:nearest_neighbor</title>
      <link href="/post/TensorFlow-Examples-NN/"/>
      <url>/post/TensorFlow-Examples-NN/</url>
      
        <content type="html"><![CDATA[<p>从<a href="https://github.com/aymericdamien/TensorFlow-Examples/">TensorFlow-Examples</a>学习TensorFlow之三：nearest_neighbor最近邻。<span id="more"></span><a href="../TensorFlow-Plan">进度</a></p><h1 id="BasicModels-nearest-neighbor-py"><a href="#BasicModels-nearest-neighbor-py" class="headerlink" title="BasicModels: nearest_neighbor.py"></a>BasicModels: nearest_neighbor.py</h1><p>最近邻的样例比较容易看明白，就是拿测试数据与训练数据一一比对距离，选最小作为预测结果。</p><ul><li>L1距离：曼哈顿距离 sum(|xi - x|)</li><li>L2距离：欧式距离  sqr(sum((xi-x)*(xi-x)))</li></ul><p>建图代码</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># tf Graph Input</span><br><span class="line">xtr = tf.placeholder(&quot;float&quot;, [None, 784])</span><br><span class="line">xte = tf.placeholder(&quot;float&quot;, [784])</span><br><span class="line"></span><br><span class="line"># Nearest Neighbor calculation using L1 Distance</span><br><span class="line"># Calculate L1 Distance</span><br><span class="line">distance = tf.reduce_sum(tf.abs(tf.add(xtr, tf.negative(xte))), reduction_indices=1)</span><br><span class="line"># Prediction: Get min distance index (Nearest neighbor)</span><br><span class="line">pred = tf.arg_min(distance, 0)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> tensorflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TF逻辑回归 预测KAGGLE:Titanic问题</title>
      <link href="/post/TensorFlow-LOR-Titanic/"/>
      <url>/post/TensorFlow-LOR-Titanic/</url>
      
        <content type="html"><![CDATA[<p>Tensorflow 逻辑回归实例</p><span id="more"></span><p><a href="https://www.kaggle.com/c/titanic">问题描述</a></p><p>使用tensorflow逻辑回归模型，用梯度下降法预测Titanic问题，只对数据进行数值化处理，得到77.9的准确率。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line">#%% </span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">from pandas import DataFrame</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(&quot;D:\\KAGGLE\\Titanic\\train.csv&quot;)</span><br><span class="line"></span><br><span class="line">def MaxMinNormalization(x):</span><br><span class="line">    Max = np.max(x)</span><br><span class="line">    Min = np.min(x)</span><br><span class="line">    result = (x - Min) / (Max - Min)</span><br><span class="line">    return result</span><br><span class="line"></span><br><span class="line"># 基本数据处理</span><br><span class="line">data[&#x27;Sex&#x27;] = data[&#x27;Sex&#x27;].apply(lambda s:1 if s == &#x27;male&#x27; else 0)</span><br><span class="line">data[&#x27;Age&#x27;] = data.Age.fillna(data.Age.mean())</span><br><span class="line">data[&#x27;Age&#x27;] = MaxMinNormalization(data[&#x27;Age&#x27;])</span><br><span class="line">data[&#x27;Fare&#x27;] = MaxMinNormalization(data[&#x27;Fare&#x27;])</span><br><span class="line"></span><br><span class="line">data[&#x27;Dead&#x27;] = data[&#x27;Survived&#x27;].apply(lambda s:int(not s))</span><br><span class="line">data = data.join(pd.get_dummies(data.Embarked, prefix=&#x27;Embarked&#x27;))</span><br><span class="line"># data.drop([&#x27;Embarked&#x27;], axis=1, inplace=True)</span><br><span class="line">data.info()</span><br><span class="line"></span><br><span class="line">data_x = data.filter(regex=&#x27;Pclass|Sex|Fare|Embarked_.*|Age|SibSp|Parch&#x27;)</span><br><span class="line"># data_x = data[[&#x27;Pclass&#x27;, &#x27;Sex&#x27;, &#x27;Fare&#x27;, &#x27;Embarked&#x27;, &#x27;Age&#x27;, &#x27;SibSp&#x27;, &#x27;Parch&#x27;]]</span><br><span class="line">data_y = data[[&#x27;Dead&#x27;, &#x27;Survived&#x27;]]</span><br><span class="line"></span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">train_X, test_X, train_Y, test_Y = train_test_split(data_x, data_y, test_size=0.2, random_state=1)</span><br><span class="line">train_X.head()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#%%</span><br><span class="line"># 训练模型</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">learning_rate = 0.003</span><br><span class="line">training_epochs = 1000</span><br><span class="line">display_step = (int)(training_epochs / 10)</span><br><span class="line">batch_size = 50</span><br><span class="line">data_width = train_X.shape[1]</span><br><span class="line">data_len = train_X.shape[0]</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32, [None, data_width])</span><br><span class="line">y = tf.placeholder(tf.float32, [None, 2])</span><br><span class="line"></span><br><span class="line">W = tf.Variable(tf.zeros([data_width, 2]))</span><br><span class="line">b = tf.Variable(tf.zeros([2]))</span><br><span class="line"></span><br><span class="line"># 预测函数</span><br><span class="line">pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax</span><br><span class="line"># loss函数</span><br><span class="line">cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"># Start training</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    # Run the initializer</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    # Training cycle</span><br><span class="line">    for epoch in range(training_epochs):</span><br><span class="line">        avg_cost = 0.</span><br><span class="line">        total_batch = int(data_len/batch_size)</span><br><span class="line">        # Loop over all batches</span><br><span class="line">        for i in range(total_batch):</span><br><span class="line">            if i == total_batch-1:</span><br><span class="line">                batch_xs = train_X[i*batch_size:data_len]</span><br><span class="line">                batch_ys = train_Y[i*batch_size:data_len]</span><br><span class="line">            else:</span><br><span class="line">                batch_xs = train_X[i*batch_size:(i+1)*batch_size]</span><br><span class="line">                batch_ys = train_Y[i*batch_size:(i+1)*batch_size]</span><br><span class="line"></span><br><span class="line">            # Run optimization op (backprop) and cost op (to get loss value)</span><br><span class="line">            _, c = sess.run([optimizer, cost], feed_dict=&#123;x: batch_xs,</span><br><span class="line">                                                          y: batch_ys&#125;)</span><br><span class="line">            # Compute average loss 平均cost</span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line"></span><br><span class="line">        # Display logs per epoch step</span><br><span class="line">        if (epoch+1) % display_step == 0:</span><br><span class="line">            print(&quot;Epoch:&quot;, &#x27;%04d&#x27; % (epoch+1), &quot;cost=&quot;, &quot;&#123;:.9f&#125;&quot;.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    print(&quot;Optimization Finished!&quot;)</span><br><span class="line"></span><br><span class="line">    # Test model</span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))</span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">    print(&quot;Accuracy:&quot;, accuracy.eval(&#123;x: test_X, y: test_Y&#125;))</span><br><span class="line"></span><br><span class="line">    # save model</span><br><span class="line">    saver.save(sess, &quot;D:\KAGGLE\Titanic\save\model.ckpt&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#%%</span><br><span class="line"># 预测结果</span><br><span class="line">data = pd.read_csv(&quot;D:\\KAGGLE\\Titanic\\test.csv&quot;)</span><br><span class="line"></span><br><span class="line">data[&#x27;Sex&#x27;] = data[&#x27;Sex&#x27;].apply(lambda s:1 if s == &#x27;male&#x27; else 0)</span><br><span class="line">data[&#x27;Age&#x27;] = data.Age.fillna(data.Age.mean())</span><br><span class="line">data[&#x27;Age&#x27;] = MaxMinNormalization(data[&#x27;Age&#x27;])</span><br><span class="line">data[&#x27;Fare&#x27;] = MaxMinNormalization(data[&#x27;Fare&#x27;])</span><br><span class="line"></span><br><span class="line">data = data.join(pd.get_dummies(data.Embarked, prefix=&#x27;Embarked&#x27;))</span><br><span class="line"></span><br><span class="line">data_x = data.filter(regex=&#x27;Pclass|Sex|Fare|Embarked_.*|Age|SibSp|Parch&#x27;)</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    # Run the initializer</span><br><span class="line">    sess.run(init)</span><br><span class="line">    # restore model</span><br><span class="line">    saver.restore(sess, &quot;D:\KAGGLE\Titanic\save\model.ckpt&quot;)</span><br><span class="line"></span><br><span class="line">    # Test model</span><br><span class="line">    prediction = sess.run(pred, feed_dict=&#123;x:data_x&#125;)</span><br><span class="line">    Survived = np.argmax(prediction, 1)</span><br><span class="line">    submission = pd.DataFrame(&#123;</span><br><span class="line">        &quot;PassengerId&quot;: data[&quot;PassengerId&quot;],</span><br><span class="line">        &quot;Survived&quot;: Survived</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    submission.to_csv(&quot;D:\\KAGGLE\\Titanic\\titanic_submission.csv&quot;, index=False)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
            <tag> kaggle </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow-Examples-LOR</title>
      <link href="/post/TensorFlow-Examples-LOR/"/>
      <url>/post/TensorFlow-Examples-LOR/</url>
      
        <content type="html"><![CDATA[<p>从<a href="https://github.com/aymericdamien/TensorFlow-Examples/">TensorFlow-Examples</a>学习TensorFlow之二：logistic_regression。<span id="more"></span><a href="../TensorFlow-Plan">进度</a></p><h1 id="BasicModels-logistic-regression-py"><a href="#BasicModels-logistic-regression-py" class="headerlink" title="BasicModels: logistic_regression.py"></a>BasicModels: logistic_regression.py</h1><blockquote><p><a href="https://baike.baidu.com/item/logistic%E5%9B%9E%E5%BD%92/2981575?fromtitle=%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92&fromid=17202449&fr=aladdin">逻辑回归是一种广义线性回归。</a></p></blockquote><p>逻辑回归同样使用线性预测方程，只是线性回归使用方程因变量作结果，而逻辑回归通过激活函数处理，将因变量转换为另一种输出；因此其与线性回归的代码结构很类似，最本质的区别可能就是，线性回归处理线性问题，而逻辑回归处理分类问题。如：</p><ul><li>已知N张数字图片和每张图片中写的数字是’0’的概率，则可将图片像素点作为自变量x，‘0’概率作为因变量y，使用线性回归去预测y&#x3D;W*x+b。</li><li>已知N张数字图片和每张图片写的数字，因变量变成了数字，不是计算机从像素点能直接得出的结果，所以数字不应该作为y。所以先构造一个y&#x3D;W*x+b，这时候y应该有10个值分别表示图片可能为i的预测值，再根据预测值比较（如最大）得出预测结果，即逻辑回归的思想。</li></ul><h2 id="在这个Example中，同样出现了线性回归样例中将测试集拆分代入优化器的情况，但是在这例中稍微能够明白这样做的原因：-在训练集固定的情况下拆分子集，能增加训练次数的同时防止模型与整个训练集过拟合。-参考：当激活函数不是0均值（即zero-centered）时，会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。-产生的一个结果就是在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果，使得收敛缓慢。-当然了，如果按batch去训练，那么那个batch可能得到不同的信号，所以这个问题还是可以缓解一下的。"><a href="#在这个Example中，同样出现了线性回归样例中将测试集拆分代入优化器的情况，但是在这例中稍微能够明白这样做的原因：-在训练集固定的情况下拆分子集，能增加训练次数的同时防止模型与整个训练集过拟合。-参考：当激活函数不是0均值（即zero-centered）时，会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。-产生的一个结果就是在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果，使得收敛缓慢。-当然了，如果按batch去训练，那么那个batch可能得到不同的信号，所以这个问题还是可以缓解一下的。" class="headerlink" title="在这个Example中，同样出现了线性回归样例中将测试集拆分代入优化器的情况，但是在这例中稍微能够明白这样做的原因：- 在训练集固定的情况下拆分子集，能增加训练次数的同时防止模型与整个训练集过拟合。- 参考：当激活函数不是0均值（即zero-centered）时，会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。 产生的一个结果就是在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果，使得收敛缓慢。 当然了，如果按batch去训练，那么那个batch可能得到不同的信号，所以这个问题还是可以缓解一下的。    "></a>在这个Example中，同样出现了<a href="../TensorFlow-Examples-LR">线性回归样例</a>中将测试集拆分代入优化器的情况，但是在这例中稍微能够明白这样做的原因：<br>- 在训练集固定的情况下拆分子集，能增加训练次数的同时防止模型与整个训练集过拟合。<br>- 参考：<a href="https://blog.csdn.net/tyhj_sf/article/details/79932893">当激活函数不是0均值（即zero-centered）时，会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。 产生的一个结果就是在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果，使得收敛缓慢。 当然了，如果按batch去训练，那么那个batch可能得到不同的信号，所以这个问题还是可以缓解一下的。 </a>   </h2><p>logistic_regression.py解读，<a href="https://github.com/yezhenbang/TensorFlow-Examples/tree/parse">代码同步更新</a></p><ul><li><a href="../TensorFlow-Note/#GradientDescentOptimizer">GradientDescentOptimizer</a>梯度下降模型</li><li><a href="../Note-1/#Cross-entropy-%E4%BA%A4%E5%8F%89%E7%86%B5">Cross-entropy 交叉熵</a></li><li><a href="../TensorFlow-LOR-Titanic">实例：预测KAGGLE:Titanic问题</a></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">A logistic regression learning algorithm example using TensorFlow library.</span><br><span class="line">This example is using the MNIST database of handwritten digits</span><br><span class="line">(http://yann.lecun.com/exdb/mnist/)</span><br><span class="line"></span><br><span class="line">Author: Aymeric Damien</span><br><span class="line">Project: https://github.com/aymericdamien/TensorFlow-Examples/</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">from __future__ import print_function</span><br><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"># Import MNIST data</span><br><span class="line"># 本例使用MNIST，数字识别数据集</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">mnist = input_data.read_data_sets(&quot;/tmp/data/&quot;, one_hot=True)</span><br><span class="line"></span><br><span class="line"># Parameters</span><br><span class="line">learning_rate = 0.01</span><br><span class="line">training_epochs = 25</span><br><span class="line">batch_size = 100</span><br><span class="line">display_step = 1</span><br><span class="line"></span><br><span class="line"># tf Graph Input</span><br><span class="line"># 数据集每个图片是28*28像素，结果是0-9共10个数字</span><br><span class="line"># None保留，可以是任何数，在这一例中后续作为每批测试数据数量(batch_size)输入</span><br><span class="line">x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784</span><br><span class="line">y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition =&gt; 10 classes</span><br><span class="line"></span><br><span class="line"># Set model weights</span><br><span class="line"># 创建变量</span><br><span class="line">W = tf.Variable(tf.zeros([784, 10]))</span><br><span class="line">b = tf.Variable(tf.zeros([10]))</span><br><span class="line"></span><br><span class="line"># Construct model</span><br><span class="line"># 回归方程 Y = x·W + b ， 可判断出Y为shape为(None, 10)的张量</span><br><span class="line"># 预测函数 pred = softmax(Y) ，对Y应用归一化指数函数，使输出概率化，</span><br><span class="line"># 得到的结果看作0-9数字的预测概率，取最大值则为预测结果。</span><br><span class="line">pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax</span><br><span class="line"></span><br><span class="line"># Minimize error using cross entropy</span><br><span class="line"># 使用交叉熵作代价函数</span><br><span class="line"># 知y(None,10),pred(None,10), y*tf.log(pred)对应位相乘得到(None,10)的张量；</span><br><span class="line"># reduce_sum对维度alix=1求和降维，效果就是sum(p*log(1/q))，得到(None)个交叉熵，</span><br><span class="line"># 即每一个单独训练集的交叉熵。</span><br><span class="line"># reduce_mean对(None)个交叉熵求平均值，得到这一次训练的平均交叉熵，并以此为代价进行优化。</span><br><span class="line">cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))</span><br><span class="line"># Gradient Descent</span><br><span class="line"># 对代价函数cost使用梯度下降优化W,b变量</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</span><br><span class="line"></span><br><span class="line"># Initialize the variables (i.e. assign their default value)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"># Start training</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line"></span><br><span class="line">    # Run the initializer</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    # Training cycle</span><br><span class="line">    # 进行training_epochs次训练</span><br><span class="line">    for epoch in range(training_epochs):</span><br><span class="line">        avg_cost = 0.</span><br><span class="line">        total_batch = int(mnist.train.num_examples/batch_size)</span><br><span class="line">        # Loop over all batches</span><br><span class="line">        # 每次取batch_size条数据，共需total_batch次</span><br><span class="line">        for i in range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            # Run optimization op (backprop) and cost op (to get loss value)</span><br><span class="line">            # 执行训练，并记录每次训练得到的cost以计算整个训练集的平均cost</span><br><span class="line">            _, c = sess.run([optimizer, cost], feed_dict=&#123;x: batch_xs,</span><br><span class="line">                                                          y: batch_ys&#125;)</span><br><span class="line">            # Compute average loss 平均cost</span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line"></span><br><span class="line">        # Display logs per epoch step</span><br><span class="line">        if (epoch+1) % display_step == 0:</span><br><span class="line">            print(&quot;Epoch:&quot;, &#x27;%04d&#x27; % (epoch+1), &quot;cost=&quot;, &quot;&#123;:.9f&#125;&quot;.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    print(&quot;Optimization Finished!&quot;)</span><br><span class="line"></span><br><span class="line">    # Test model</span><br><span class="line">    # argmax返回pred在维度alis=1上最大值的下标，pred是(None,10)，所以得到(None)个预测值（概率最高）</span><br><span class="line">    # correct_prediction就是(None)个bool，是否预测正确</span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))</span><br><span class="line">    # Calculate accuracy 准确率</span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">    # 将MNIST的测试数据代入，计算准确率</span><br><span class="line">    # .eval 与 sess.run(accuracy)相同</span><br><span class="line">    print(&quot;Accuracy:&quot;, accuracy.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> tensorflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Note 概论信息论</title>
      <link href="/post/Note-1/"/>
      <url>/post/Note-1/</url>
      
        <content type="html"><![CDATA[<p>..</p><span id="more"></span><h2 id="Mean-squared-error-均方误差"><a href="#Mean-squared-error-均方误差" class="headerlink" title="Mean-squared-error 均方误差"></a>Mean-squared-error 均方误差</h2><blockquote><p>cost &#x3D; tf.reduce_sum(tf.pow(pred-Y, 2))&#x2F;(2*n_samples)</p></blockquote><p>可用于代价函数。线性问题中似乎比较受用。</p><p>均方误差 cost &#x3D; E(pred - Y)^2 &#x2F; 2， 1&#x2F;2系数使得平方求梯度后常数系数为1，方便计算，系数对结果不影响。</p><h2 id="Cross-entropy-交叉熵"><a href="#Cross-entropy-交叉熵" class="headerlink" title="Cross-entropy 交叉熵"></a>Cross-entropy 交叉熵</h2><blockquote><p>cost &#x3D; tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices&#x3D;1))</p></blockquote><p>度量两个概率分布间的差异性，可用于代价函数。逻辑分类问题似乎比较受用。</p><img src="https://latex.codecogs.com/gif.latex?h(p,q)=\sum_i&space;p_i&space;\log&space;\frac{1}{q_i}" title="h(p,q)=\sum_i p_i \log \frac{1}{q_i}" /><h2 id="Softmax-归一化指数函数"><a href="#Softmax-归一化指数函数" class="headerlink" title="Softmax 归一化指数函数"></a>Softmax 归一化指数函数</h2><blockquote><p>将一个含任意实数的K维向量“压缩”到另一个K维实向量中，使得每一个元素的范围都在(0,1)之间，并且所有元素的和为1。 –百度百科</p></blockquote><blockquote><p>tf.nn.softmax(tf.matmul(x, W) + b)</p></blockquote><p>可用于多分类问题的输出。在处理二分类时，可使用交叉熵作为损失函数。</p><h2 id="Sigmoid-函数"><a href="#Sigmoid-函数" class="headerlink" title="Sigmoid 函数"></a>Sigmoid 函数</h2><p>同样将元素压缩到(0,1)，但只对一个实数生效。可用于处理二分类问题。</p><p><img src="/softmax1.png"></p><img src="/post/Note-1/softmax1.png" class="" title="softmax与sigmoid异同--百度百科"><p>扩展：<a href="https://blog.csdn.net/u011684265/article/details/78039280">激活函数的比较和优缺点</a></p>]]></content>
      
      
      <categories>
          
          <category> Note </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>git note</title>
      <link href="/post/Note-Git/"/>
      <url>/post/Note-Git/</url>
      
        <content type="html"><![CDATA[<p>note to git..</p><span id="more"></span><h2 id="修改账户"><a href="#修改账户" class="headerlink" title="修改账户"></a>修改账户</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name &quot;username&quot;</span><br><span class="line"> </span><br><span class="line">git config --global user.email &quot;email&quot;</span><br></pre></td></tr></table></figure><p>这个配置修改提交代码的贡献者。</p><p>如果更换了账号且使用https的情况下，push还是会使用原账号，出现权限失败问题。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">remote: Permission to *.git denied to *.</span><br><span class="line">fatal: unable to access &#x27;https://github.com/*.git/&#x27;: The requested URL returned error: 403</span><br></pre></td></tr></table></figure><ul><li>编辑<code>控制面板-用户-Windows凭据</code>下的<code>git:https://github.com</code>账号及密码即可。</li><li>改用ssh方式即可。</li></ul><h2 id="fork仓库保持原仓库一致"><a href="#fork仓库保持原仓库一致" class="headerlink" title="fork仓库保持原仓库一致"></a>fork仓库保持原仓库一致</h2><p>添加一个remote源，使用该源来更新代码即可</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git remote add upstream &quot;origin.git&quot;</span><br><span class="line">git fetch upstream</span><br><span class="line">git merge upstream/master</span><br></pre></td></tr></table></figure><h2 id="数学表达式"><a href="#数学表达式" class="headerlink" title="数学表达式"></a>数学表达式</h2><p><a href="https://www.codecogs.com/latex/eqneditor.php">https://www.codecogs.com/latex/eqneditor.php</a></p>]]></content>
      
      
      <categories>
          
          <category> NOTE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++11 chrono库获取时间 精度</title>
      <link href="/post/C-chrono-time-test/"/>
      <url>/post/C-chrono-time-test/</url>
      
        <content type="html"><![CDATA[<p>通过菜鸟测试分析chrono的精确度</p><span id="more"></span><blockquote><p>1秒(s) &#x3D; 100分秒(ds) &#x3D; 1000 毫秒(ms) &#x3D; 1e6 微秒(μs) &#x3D; 1e9 纳秒(ns) &#x3D; 1e12 皮秒(ps) &#x3D; 1e15 飞秒(fs) &#x3D; 1e18 阿秒(as) &#x3D; 1e21 仄秒(zs) &#x3D; 1e24 幺秒(ys)</p></blockquote><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>因为在项目中使用了C++11 chrono库的system_clock：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#define GET_TIME_STAMP std::chrono::duration_cast&lt;std::chrono::milliseconds&gt;(std::chrono::system_clock::now().time_since_epoch()).count()</span><br></pre></td></tr></table></figure><p>来获取时间戳用以计时，看着很复杂，有时间来分析一下chrono的精确度。</p><h2 id="铺垫"><a href="#铺垫" class="headerlink" title="铺垫"></a>铺垫</h2><p>语言层面上的时间函数都或多或少存在误差，翻阅资料后决定通过计算机的高精度计数器HPET来计算运行时间。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">BOOL QueryPerformanceFrequency(LARGE_INTEGER *lpFrequency);</span><br><span class="line">// 获取高精度计数器的时钟频率</span><br><span class="line">BOOL QueryPerformanceCounter (LARGE_INTEGER *lpCount);</span><br><span class="line">// 获取从上电以来计数器计数</span><br><span class="line"></span><br><span class="line">during = (Count_end.QuadPart - Count_begin.QuadPart) / freq.QuadPart * 1e9;</span><br><span class="line">// 获取以纳秒为单位的时间</span><br></pre></td></tr></table></figure><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>了解过程中发现chrono除了system_clock还有一个steady_clock，有了以下测试代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;Windows.h&gt;</span><br><span class="line">#include &lt;chrono&gt;</span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;iomanip&gt;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">LARGE_INTEGER HPET_begin, HPET_end;</span><br><span class="line">auto t_sysclk_begin = chrono::system_clock::now();</span><br><span class="line">auto t_steadyclk_begin = chrono::steady_clock::now();</span><br><span class="line">QueryPerformanceCounter(&amp;HPET_begin);</span><br><span class="line"></span><br><span class="line">Sleep(1000);</span><br><span class="line"></span><br><span class="line">auto t_sysclk_end = chrono::system_clock::now();</span><br><span class="line">auto t_steadyclk_end = chrono::steady_clock::now();</span><br><span class="line">QueryPerformanceCounter(&amp;HPET_end);</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; fixed;</span><br><span class="line">cout &lt;&lt; setprecision(0);</span><br><span class="line"></span><br><span class="line">auto d = chrono::duration_cast&lt;chrono::nanoseconds&gt;(t_sysclk_end - t_sysclk_begin);</span><br><span class="line">// 计算精度设置chrono::nanoseconds，还有microseconds ~ hours，方便！</span><br><span class="line">cout &lt;&lt; &quot;chrono::system_clock: &quot; &lt;&lt; d.count() &lt;&lt; &quot;ns&quot; &lt;&lt; endl;</span><br><span class="line">auto d3 = chrono::duration_cast&lt;chrono::nanoseconds&gt;(t_steadyclk_end - t_steadyclk_begin);</span><br><span class="line">cout &lt;&lt; &quot;chrono::steady_clock: &quot; &lt;&lt; d3.count() &lt;&lt; &quot;ns&quot; &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">LARGE_INTEGER freq;</span><br><span class="line">QueryPerformanceFrequency(&amp;freq);</span><br><span class="line">cout &lt;&lt; &quot;HPET clock:           &quot; &lt;&lt; (double)(HPET_end.QuadPart - HPET_begin.QuadPart) / freq.LowPart * 1e9&lt;&lt; &quot;ns&quot; &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; &quot;HPET freq: &quot; &lt;&lt; freq.LowPart &lt;&lt; &quot;; 理论误差：&quot; &lt;&lt; 1.0 / freq.LowPart * 1e9 &lt;&lt; &quot;ns&quot; &lt;&lt; endl;</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>中间用Sleep(1000)延时1秒，以计时，输出如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">chrono::system_clock: 1000613300ns</span><br><span class="line">chrono::steady_clock: 1000610323ns</span><br><span class="line">HPET clock:           1000610689ns</span><br><span class="line">HPET freq: 2728067; 理论误差：367ns</span><br></pre></td></tr></table></figure><p>理论误差是指HPET clock获取的时间理想情况下误差在一次计数的时间（1&#x2F;freq）以内。</p><p>从输出结果可以看到</p><p><strong>chrono::system_clock的精度是100ns</strong>，</p><p>而<strong>chrono::steady_clock的精度达到了1ns</strong>。 </p><h2 id="推测"><a href="#推测" class="headerlink" title="推测"></a>推测</h2><p>通过测试发现steady_clock得到的值多次与HPET计算值相同，其余都是相差一个误差，因此推测steady_clock也是通过高精度计数器计算的。</p><p>但是网上找不到相关的说法，只好看:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">// chrono.h</span><br><span class="line">struct steady_clock</span><br><span class="line">&#123;// wraps QueryPerformanceCounter</span><br><span class="line">……</span><br><span class="line">_NODISCARD static time_point now() noexcept</span><br><span class="line">&#123;// get current time</span><br><span class="line">const long long _Freq = _Query_perf_frequency();// doesn&#x27;t change after system boot</span><br><span class="line">const long long _Ctr = _Query_perf_counter();</span><br><span class="line">……</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">using high_resolution_clock = steady_clock;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>调用了MS的<code>_Query_perf_counter(); </code> 看不到具体实现，网上居然也找不到直接的介绍，猜测跟<code>QueryPerformanceCounter</code>有关，保留猜测。</p><p>其实chrono还有一个clock：high_resolution_clock </p><blockquote><p>using high_resolution_clock &#x3D; steady_clock;</p></blockquote><p>其实就是steady_clock。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>chrono::system_clock 可以获取时钟时间，精度到100ns。</p><p>chrono::steady_clock 形似高精度计数器，精度1ns。稳定增加，是chrono指定<code>**適於度量間隔**</code>的clock。</p><hr><h2 id="附加"><a href="#附加" class="headerlink" title="附加"></a>附加</h2><p>偶然发现boost库有一个计时器cpu_timer，顺便测了一下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;boost/timer/timer.hpp&gt;</span><br><span class="line">#include &lt;Windows.h&gt;</span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;iomanip&gt;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">boost::timer::cpu_timer boost_timer;</span><br><span class="line">LARGE_INTEGER cpu_begin, cpu_end;</span><br><span class="line">boost_timer.start();</span><br><span class="line">QueryPerformanceCounter(&amp;cpu_begin);</span><br><span class="line"></span><br><span class="line">Sleep(1000);</span><br><span class="line"></span><br><span class="line">boost_timer.stop();</span><br><span class="line">QueryPerformanceCounter(&amp;cpu_end);</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; fixed;</span><br><span class="line">cout &lt;&lt; setprecision(0);</span><br><span class="line">LARGE_INTEGER freq;</span><br><span class="line">QueryPerformanceFrequency(&amp;freq);</span><br><span class="line">cout &lt;&lt; &quot;Boost::cpu_timer: &quot; &lt;&lt; boost_timer.format(9) &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; &quot;Cpu clock:            &quot; &lt;&lt; (double)(cpu_end.QuadPart - cpu_begin.QuadPart) / freq.LowPart * 1e9&lt;&lt; &quot;ns&quot; &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; &quot;cpu freq: &quot; &lt;&lt; freq.LowPart &lt;&lt; &quot;; 理论误差：&quot; &lt;&lt; 1.0 / freq.LowPart * 1e9 &lt;&lt; &quot;ns&quot; &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Boost::cpu_timer:    1.000058283s wall, 0.00s user + 0.00s system = 0.00s CPU (n/a%)</span><br><span class="line">Cpu clock:            1000061949ns</span><br><span class="line">cpu freq: 2728067; 理论误差：367ns</span><br></pre></td></tr></table></figure><p>比起自己通过chrono获取时间算的误差大多了，足足有 4-10us，官方倒是对此有解释：</p><blockquote><p>Intel Core – 366ns –  Some variation, usually in multiples of 366ns<br> <a href="https://www.boost.org/doc/libs/1_48_0/libs/timer/doc/cpu_timers.html">reference</a></p></blockquote><p>通常是366ns的倍数。</p>]]></content>
      
      
      <categories>
          
          <category> NOTE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow-Examples:linear_regression</title>
      <link href="/post/TensorFlow-Examples-LR/"/>
      <url>/post/TensorFlow-Examples-LR/</url>
      
        <content type="html"><![CDATA[<p>从<a href="https://github.com/aymericdamien/TensorFlow-Examples/">TensorFlow-Examples</a>学习TensorFlow之basic_operations和linear_regression。<span id="more"></span><a href="../TensorFlow-Plan">进度</a></p><h1 id="basic-operations-py"><a href="#basic-operations-py" class="headerlink" title="basic_operations.py"></a>basic_operations.py</h1><ul><li><p><a href="../TensorFlow-Note/#tf-constant">tf.constant</a> 创建常量</p></li><li><p><a href="../TensorFlow-Note/#tf-placeholder">tf.placeholder</a> 创建占位符</p></li><li><p><a href="https://www.tensorflow.org/api_docs/python/tf/linalg/matmul">tf.matmul</a> 矩阵相乘</p></li></ul><h1 id="BasicModels-linear-regression-py"><a href="#BasicModels-linear-regression-py" class="headerlink" title="BasicModels: linear_regression.py"></a>BasicModels: linear_regression.py</h1><p>线性回归：使用线性模型（如一[多]元一次方程）去逼近数据模型，一般可用均方误差当损失函数，通过梯度下降法逼近最优解。</p><p>样例使用梯度下降法求解线性回归。</p><h2 id="梯度下降法原理"><a href="#梯度下降法原理" class="headerlink" title="梯度下降法原理"></a>梯度下降法原理</h2><p>通过给模型定义代价函数（如均方误差）来表示模型与数据的拟合程度，则代价函数值越小，拟合程度越高。</p><p>因此通过代价函数loss的偏导得到loss在当前状态自变量往极小值收缩的方向，从而调整更新自变量的值，足够次数之后总会逼近极小点，学习率用以调整更新的密度，越小，精度越高次数越多。</p><p>多元情况下，注意某个自变量值太分散可应用归一化。</p><p>可参考详细资料：<a href="https://www.jianshu.com/p/c7e642877b0e">https://www.jianshu.com/p/c7e642877b0e</a></p><p>linear_regression.py解读，<a href="https://github.com/yezhenbang/TensorFlow-Examples/tree/parse">代码同步更新</a></p><ul><li><a href="../TensorFlow-Note/#GradientDescentOptimizer">GradientDescentOptimizer</a>梯度下降模型</li></ul><hr><p><em>Q:执行优化器时为什么要将数据用循环一个个代入？存疑</em> </p><p><em>A:（猜测）在训练集固定的情况下拆分子集，能增加训练次数的同时防止模型与整个训练集过拟合。。把点一个个代入线性模型，是什么道理？存疑</em></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">A linear regression learning algorithm example using TensorFlow library.</span><br><span class="line"></span><br><span class="line">Author: Aymeric Damien</span><br><span class="line">Project: https://github.com/aymericdamien/TensorFlow-Examples/</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">#%%</span><br><span class="line">from __future__ import print_function</span><br><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy</span><br><span class="line">import matplotlib.pyplot as plt </span><br><span class="line">rng = numpy.random</span><br><span class="line"></span><br><span class="line"># Parameters</span><br><span class="line">learning_rate = 0.02</span><br><span class="line">training_epochs = 1500</span><br><span class="line">display_step = 100</span><br><span class="line"></span><br><span class="line"># Training Data</span><br><span class="line"># asarray：转换为ndarray对象</span><br><span class="line">train_X = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,</span><br><span class="line">                         7.042,10.791,5.313,7.997,5.654,9.27,3.1])</span><br><span class="line">train_Y = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,</span><br><span class="line">                         2.827,3.465,1.65,2.904,2.42,2.94,1.3])</span><br><span class="line">n_samples = train_X.shape[0]</span><br><span class="line"></span><br><span class="line"># tf Graph Input</span><br><span class="line"># 声明float32类型，未定义形状（shape）</span><br><span class="line">X = tf.placeholder(&quot;float&quot;)</span><br><span class="line">Y = tf.placeholder(&quot;float&quot;)</span><br><span class="line">#print(X)</span><br><span class="line"></span><br><span class="line"># Set model weights</span><br><span class="line"># 创建变量W和b，作为线性方程的参数，并初始化为float随机数</span><br><span class="line">W = tf.Variable(rng.randn(), name=&quot;weight&quot;, trainable=True)</span><br><span class="line">b = tf.Variable(rng.randn(), name=&quot;bias&quot;)</span><br><span class="line"></span><br><span class="line"># Construct a linear model</span><br><span class="line"># 线性模型 pred = WX + b</span><br><span class="line">pred = tf.add(tf.multiply(X, W), b)</span><br><span class="line"></span><br><span class="line"># Mean squared error</span><br><span class="line"># 均方误差 cost = E(pred - Y)^2 表示梯度下降中的代价函数，值越小表示越拟合数据</span><br><span class="line"># 1/2系数使得平方求梯度后常数系数为1，方便计算，系数对结果不影响</span><br><span class="line">cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)</span><br><span class="line"></span><br><span class="line"># Gradient descent</span><br><span class="line">#  Note, minimize() knows to modify W and b because Variable objects are trainable=True by default</span><br><span class="line"># 使用梯度下降算法模型优化器。minimize(cost)包括compute_gradients(cost)和apply_gradients()。</span><br><span class="line"># compute_gradients(cost)计算cost的梯度，默认使用GraphKeys.TRAINABLE_VARIABLES，所以包括了变量W和b</span><br><span class="line"># apply_gradients()应用梯度到变量列表，变量W和b为trainable，在这一步更新变量W和b的值</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</span><br><span class="line"></span><br><span class="line"># Initialize the variables (i.e. assign their default value)</span><br><span class="line"># 初始化变量的操作</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"># Start training</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line"></span><br><span class="line">    # Run the initializer</span><br><span class="line">    # 初始化，初始化变量</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    # Fit all training data</span><br><span class="line">    for epoch in range(training_epochs): #训练次数</span><br><span class="line">        for (x, y) in zip(train_X, train_Y): #将X,Y打包成(xi,yi)对的形式</span><br><span class="line">            sess.run(optimizer, feed_dict=&#123;X: x, Y: y&#125;) #将每组(xi,yi)代入优化器计算</span><br><span class="line"></span><br><span class="line">        # 用下面这句替代上面一层循环，得到接近的结果，但速度明显提升，为什么不这样用，存疑。</span><br><span class="line">        # sess.run(optimizer, feed_dict=&#123;X: train_X, Y:train_Y&#125;)</span><br><span class="line"></span><br><span class="line">        # Display logs per epoch step</span><br><span class="line">        # 每display_step次输出状态</span><br><span class="line">        if (epoch+1) % display_step == 0:</span><br><span class="line">            c = sess.run(cost, feed_dict=&#123;X: train_X, Y:train_Y&#125;)</span><br><span class="line">            print(&quot;Epoch:&quot;, &#x27;%04d&#x27; % (epoch+1), &quot;cost=&quot;, &quot;&#123;:.9f&#125;&quot;.format(c), \</span><br><span class="line">                &quot;W=&quot;, sess.run(W), &quot;b=&quot;, sess.run(b))</span><br><span class="line"></span><br><span class="line">    print(&quot;Optimization Finished!&quot;)</span><br><span class="line">    training_cost = sess.run(cost, feed_dict=&#123;X: train_X, Y: train_Y&#125;)</span><br><span class="line">    print(&quot;Training cost=&quot;, training_cost, &quot;W=&quot;, sess.run(W), &quot;b=&quot;, sess.run(b), &#x27;\n&#x27;)</span><br><span class="line"></span><br><span class="line">    # Graphic display</span><br><span class="line">    # 图形化等</span><br><span class="line">    plt.plot(train_X, train_Y, &#x27;ro&#x27;, label=&#x27;Original data&#x27;)</span><br><span class="line">    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label=&#x27;Fitted line&#x27;)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    # Testing example, as requested (Issue #2)</span><br><span class="line">    test_X = numpy.asarray([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])</span><br><span class="line">    test_Y = numpy.asarray([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])</span><br><span class="line"></span><br><span class="line">    print(&quot;Testing... (Mean square loss Comparison)&quot;)</span><br><span class="line">    testing_cost = sess.run(</span><br><span class="line">        tf.reduce_sum(tf.pow(pred - Y, 2)) / (2 * test_X.shape[0]),</span><br><span class="line">        feed_dict=&#123;X: test_X, Y: test_Y&#125;)  # same function as cost above</span><br><span class="line">    print(&quot;Testing cost=&quot;, testing_cost)</span><br><span class="line">    print(&quot;Absolute mean square loss difference:&quot;, abs(</span><br><span class="line">        training_cost - testing_cost))</span><br><span class="line"></span><br><span class="line">    plt.plot(test_X, test_Y, &#x27;bo&#x27;, label=&#x27;Testing data&#x27;)</span><br><span class="line">    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label=&#x27;Fitted line&#x27;)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> tensorflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow Note</title>
      <link href="/post/TensorFlow-Note/"/>
      <url>/post/TensorFlow-Note/</url>
      
        <content type="html"><![CDATA[<p>TensorFlow study note #1.</p><span id="more"></span><h1 id="One-Basic"><a href="#One-Basic" class="headerlink" title="One | Basic"></a>One | Basic</h1><h2 id="Graph"><a href="#Graph" class="headerlink" title="Graph"></a><a href="https://www.tensorflow.org/api_docs/python/tf/Grap" title="tensorflow.org - Graph">Graph</a></h2><blockquote><p><a href="https://www.tensorflow.org/api_docs/python/tf/Grap" title="tensorflow.org - Graph">A Graph contains a set of tf.Operation objects, which represent units of computation; and tf.Tensor objects, which represent the units of data that flow between operations.</a></p></blockquote><p><a href="https://www.tensorflow.org/api_docs/python/tf/Grap" title="tensorflow.org - Graph">Graph</a>包含一系列操作(op)和张量(tensor，op输出的标识符)，是一系列计算过程的合集。TensorFlow全局有一默认Graph，不需要手动创建。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 在默认Graph下</span><br><span class="line">m1 = tf.constant([[3,5],[3,5]])</span><br><span class="line">m2 = tf.constant([[2,4],[2,4]])</span><br><span class="line">result = tf.add(m1,m2)</span><br><span class="line"></span><br><span class="line"># 创建新Graph g1</span><br><span class="line">g1 = tf.Graph()</span><br><span class="line">with g.as_default():</span><br><span class="line">  m3 = tf.constant(30)</span><br></pre></td></tr></table></figure><p>代码中默认Graph可表示为下图：tensor为边表示输出，op为节点表示操作。<a href="">constant</a>和<a href="">add</a>是op，constant创建了张量m1和m2，add将m1和m2相加并产生新张量。但Graph只定义了如何操作，并不会真正执行。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">m1(m1) --&gt; add(add)</span><br><span class="line">m2(m2) --&gt; add</span><br></pre></td></tr></table></figure><hr><h2 id="Session"><a href="#Session" class="headerlink" title="Session"></a><a href="https://www.tensorflow.org/api_docs/python/tf/Session" title="tensorflow.org - session">Session</a></h2><blockquote><p><a href="https://www.tensorflow.org/api_docs/python/tf/Session" title="tensorflow.org - session">A Session object encapsulates the environment in which Operation objects are executed, and Tensor objects are evaluated. </a></p></blockquote><p><a href="https://www.tensorflow.org/api_docs/python/tf/Session" title="tensorflow.org - session">Session</a>对话是定义tensor和执行op的环境，在Session中可指定要执行的Graph中的op。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 默认Session使用默认Graph</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    print(sess.run(result))   #输出： [[5 9] [5 9]]</span><br><span class="line"></span><br><span class="line"># 指定使用g1</span><br><span class="line">with tf.Session(graph=g1) as sess:</span><br><span class="line">    print(sess.run(m3))   #输出： 30</span><br><span class="line">    print(sess.run(m1))    #出错，g1中没有定义m1</span><br><span class="line"></span><br><span class="line"># placeholder 和 run(feed_dict=)</span><br><span class="line">a = tf.placeholder(tf.int16)</span><br><span class="line">b = tf.placeholder(tf.int16)</span><br><span class="line">add = tf.add(a, b)</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    print(&quot;Addition with variables: %i&quot; % sess.run(add, feed_dict=&#123;a: 2, b: 3&#125;))</span><br></pre></td></tr></table></figure><hr><h2 id="Op"><a href="#Op" class="headerlink" title="Op"></a><a href="https://www.tensorflow.org/api_docs/python/tf/Operation" title="tensorflow.org - Op">Op</a></h2><blockquote><p><a href="https://www.tensorflow.org/api_docs/python/tf/Operation" title="tensorflow.org - Op">An Operation is a node in a TensorFlow Graph that takes zero or more Tensor objects as input, and produces zero or more Tensor objects as output. </a></p></blockquote><p>Op是Graph的节点，输入tensor输出tensor。</p><p><strong>以下收集op笔记</strong>：</p><h3 id="tf-constant"><a href="#tf-constant" class="headerlink" title="tf.constant"></a><a href="https://www.tensorflow.org/api_docs/python/tf/constant">tf.constant</a></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensor = tf.constant([1, 2, 3]) =&gt; [1 2 3]</span><br><span class="line"></span><br><span class="line">tensor = tf.constant(-1.0, shape=[2, 3]) =&gt; [[-1. -1. -1.]</span><br><span class="line">                                             [-1. -1. -1.]]</span><br><span class="line"></span><br><span class="line">m = tf.constant([[1,2,3],[4,5,6]], shape=[3,3], verify_shape=True)   # Error: shape</span><br><span class="line"></span><br><span class="line">m = tf.constant([[1,2,3],[4,5,6]], shape=[3,3]) =&gt; [[1 2 3]</span><br><span class="line">                                                    [4 5 6]</span><br><span class="line">                                                    [6 6 6]]</span><br></pre></td></tr></table></figure><blockquote><p><a href="https://www.cnblogs.com/jiaxblog/p/9054051.html">tf.constant()是直接定义在graph里的，它是graph的一部分，会随着graph一起加载。如果通过tf.constant()定义了一个维度很高的张量，那么graph占用的内存就会变大，加载也会变慢。而tf.placeholder就没有这个问题，所以如果数据维度很高的话，定义成tf.placeholder是更好的选择。</a> –未求证</p></blockquote><h3 id="tf-placeholder"><a href="#tf-placeholder" class="headerlink" title="tf.placeholder"></a><a href="https://www.tensorflow.org/api_docs/python/tf/placeholder" title="tensorflow.org - placeholder">tf.placeholder</a></h3><p>占位符，类似于声明。不指定shape可填充任意shape。</p><blockquote><p><a href="https://www.tensorflow.org/api_docs/python/tf/placeholder" title="tensorflow.org - placeholder">Its value must be fed using the feed_dict optional argument to Session.run(), Tensor.eval(), or Operation.run().</a></p><p>使用时必须用feed_dict选修填充数据。参考<a href="#session">session</a></p></blockquote><h3 id="reduce-sum"><a href="#reduce-sum" class="headerlink" title="reduce_sum"></a><a href="https://www.tensorflow.org/api_docs/python/tf/math/reduce_sum" title="tensorflow.org - reduce_sum">reduce_sum</a></h3><p>通过求和降维。</p><p>判断要降哪一维：若M的shape为(6,7,8)，即6<em>7</em>8的三维矩阵，则alis&#x3D;0,1,2分别得到(7<em>8),(6</em>8),(6*7)的二维矩阵，alis&#x3D;[1,2]则先把7去掉，再把8去掉，最终得到(6)的一维向量，若再alis&#x3D;[1,2,0]则6也去掉，从向量变成一个点（一个数字）。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 官方例子</span><br><span class="line">x = tf.constant([[1, 1, 1], [1, 1, 1]])</span><br><span class="line">tf.reduce_sum(x)  # 6</span><br><span class="line">tf.reduce_sum(x, 0)  # [2, 2, 2]</span><br><span class="line">tf.reduce_sum(x, 1)  # [3, 3]</span><br><span class="line">tf.reduce_sum(x, 1, keepdims=True)  # [[3], [3]]</span><br><span class="line">tf.reduce_sum(x, [0, 1])  # 6</span><br></pre></td></tr></table></figure><p>参数</p><ul><li>alis(reduction_indices) - 要降低的维度，支持多个如[0,1] <code>The dimensions to reduce. If None (the default), reduces all dimensions. Must be in the range [-rank(input_tensor), rank(input_tensor)).</code></li></ul><h3 id="tf-nn-softmax"><a href="#tf-nn-softmax" class="headerlink" title="tf.nn.softmax"></a><a href="https://www.tensorflow.org/api_docs/python/tf/nn/softmax">tf.nn.softmax</a></h3><p>归一化指数函数</p><p>使得每一个元素的范围都在(0,1)之间，并且所有元素的和为1。</p><hr><h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a><a href="https://www.tensorflow.org/api_docs/python/tf/Tensor" title="tensorflow.org - Tensor">Tensor</a></h2><blockquote><p>A <a href="https://www.tensorflow.org/api_docs/python/tf/Tensor" title="tensorflow.org - Tensor">Tensor</a> is a symbolic handle to one of the outputs of an Operation. It does not hold the values of that operation’s output, but instead provides a means of computing those values in a TensorFlow tf.Session.</p></blockquote><p>Tensor是对操作输出的标识处理，它并不代表输出值的本身，而是为Session提供计算那些值的手段。就是标识符的意思吧…</p><p><strong>以下收集tensor笔记</strong>：</p><hr><p><em>hint: 以下基本属于class，基于个人理解放在这一章。</em></p><h3 id="tf-Variable"><a href="#tf-Variable" class="headerlink" title="tf.Variable"></a><a href="https://www.tensorflow.org/api_docs/python/tf/Variable" title="tensorflow.org - Variable">tf.Variable</a></h3><blockquote><p><a href="https://www.tensorflow.org/api_docs/python/tf/Variable" title="tensorflow.org - Variable">A tf.Variable represents a tensor whose value can be changed by running ops on it. Unlike tf.Tensor objects, a tf.Variable exists outside the context of a single session.run call.</a></p><p>Variable表示值可变的tensor，存在于session.run()上下文之外。 （意思是全局变量？）</p></blockquote><ul><li><p>创建 <code>W = tf.Variable(rng.randn(), name=&quot;weight&quot;)</code></p></li><li><p>在session中使用前需 <code>sess.run(W.initializer)</code>或 <code>sess.run(tf.global_variables_initializer()) </code>初始化值，会通过创建语句的值（这里是rng.randn()随机数）初始化变量。</p></li><li><p>trainable &#x3D; default-True</p></li></ul><hr><h2 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a><a href="https://www.tensorflow.org/api_docs/python/tf/train/Optimizer" title="tensorflow.org - Optimizer">Optimizer</a></h2><blockquote><p><a href="https://www.tensorflow.org/api_docs/python/tf/train/Optimizer" title="tensorflow.org - Optimizer">This class defines the API to add Ops to train a model. </a></p><p>优化器 基类，定义训练模型的接口和操作。</p></blockquote><p>补充资料：<a href="http://ruder.io/optimizing-gradient-descent/">http://ruder.io/optimizing-gradient-descent/</a></p><h3 id="GradientDescentOptimizer"><a href="#GradientDescentOptimizer" class="headerlink" title="GradientDescentOptimizer"></a><a href="https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer" title="tensorflow.org - GradientDescentOptimizer">GradientDescentOptimizer</a></h3><p>梯度下降优化器 <code>tf.train.GradientDescentOptimizer</code></p><p>Args:</p><ul><li>learning_rate: A Tensor or a floating point value. 学习率</li><li>use_locking: If True use locks for update operations. 更新锁</li><li>name: Optional name prefix for the operations created when applying gradients. Defaults to “GradientDescent”.</li></ul><blockquote><p>.minimize() simply combines calls compute_gradients() and apply_gradients().</p><p> minimize包括了compute_gradients()和apply_gradients().</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> tensorflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++ skills record</title>
      <link href="/post/Note-C-skills/"/>
      <url>/post/Note-C-skills/</url>
      
        <content type="html"><![CDATA[<p>Note from daily use.</p><span id="more"></span><h1 id="C-11"><a href="#C-11" class="headerlink" title="C++11"></a>C++11</h1><h2 id="decltype"><a href="#decltype" class="headerlink" title="decltype"></a>decltype</h2><p>字面意思类型获取，可通过表达式分析出结果类型。</p><p>参考：<a href="https://www.cnblogs.com/cauchy007/p/4966485.html">https://www.cnblogs.com/cauchy007/p/4966485.html</a></p><h3 id="引申：追踪返回类型"><a href="#引申：追踪返回类型" class="headerlink" title="引申：追踪返回类型"></a>引申：追踪返回类型</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">template &lt;typename T1, typename T2&gt;</span><br><span class="line">auto sum(T1&amp; t1, T2&amp; t2) -&gt; decltype(t1 + t2) &#123;</span><br><span class="line">    return t1 + t2;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>编译通过，执行时根据<code>decltype(t1 + t2)</code>判断类型，结合auto实现自动判断函数返回类型。</p><hr><h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="宏"><a href="#宏" class="headerlink" title="宏"></a>宏</h2><ul><li><p>ANSI C标准中有几个标准预定义宏（也是常用的）：</p><p>  <code>__LINE__</code>：在源代码中插入当前源代码行号；</p><p>  <code>__FILE__</code>：在源文件中插入当前源文件名；</p><p>  <code>__DATE__</code>：在源文件中插入当前的编译日期</p><p>  <code>__TIME__</code>：在源文件中插入当前编译时间；</p><p>  <code>__STDC_</code>_&#96;：当要求程序严格遵循ANSI C标准时该标识被赋值为1；</p><p>  <code>__cplusplus</code>：当编写C++程序时该标识符被定义。</p><p>  <strong>以上宏在编译时被替换，像DATE、TIME是被固定的</strong>，企图用在日志名字是不现实的</p></li><li><p><code>##</code>允许可变参数为空，如<code>#define CLOG(format, ...) consoleString(format, ##__VA_ARGS__);</code>，不加<code>##</code>时若CLOG(“haha”)会出错。</p></li></ul><h2 id="char-amp-string"><a href="#char-amp-string" class="headerlink" title="char &amp; string"></a>char &amp; string</h2><h3 id="TRUNCATE"><a href="#TRUNCATE" class="headerlink" title="_TRUNCATE"></a>_TRUNCATE</h3><p><a href="https://msdn.microsoft.com/zh-cn/library/ms175769%28v=vs.140%29?f=255&MSPPError=-2147217396">_TRUNCATE</a> 指定字符串截断行为；尽可能保留字符串并不产生错误。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">char src[] = &quot;1234567890&quot;;</span><br><span class="line">char dst[dst_len];</span><br><span class="line">errno_t err = _snprintf_s(dst, _countof(dst), _TRUNCATE, src);</span><br></pre></td></tr></table></figure><ul><li>dst_len &gt; strlen(src) , err &#x3D; strlen(src);</li><li>dst_len &lt;&#x3D; strlen(src)<ul><li>不使用_TRUNCATE，程序奔溃出错；</li><li>使用_TRUNCATE，err &#x3D; -1，dst保留最大字符串。</li></ul></li></ul><p>使用 <code>err = strncpy_s(dst, _countof(dst), src, _TRUNCATE);</code> 情况相同；</p><ul><li>dst_len &gt; strlen(src) , err &#x3D; 0;</li><li>dst_len &lt;&#x3D; strlen(src)<ul><li>不使用_TRUNCATE，使用_countof(src)，程序奔溃出错；</li><li>使用_TRUNCATE，err &#x3D; STRUNCATE，dst保留最大字符串。</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> NOTE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/post/hello-world/"/>
      <url>/post/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><span id="more"></span><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html">Deployment</a></p>]]></content>
      
      
      <categories>
          
          <category> study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> test </tag>
            
            <tag> default </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
